<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Bhavin Jawade</title>
    <link>https://www.bhavinjawade.github.io/tags/llm/</link>
    <description>Recent content in LLM on Bhavin Jawade</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 13 Jul 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.bhavinjawade.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Understanding MatFormer - Nested Transformers for elastic inference</title>
      <link>https://www.bhavinjawade.github.io/post/matformers/</link>
      <pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.bhavinjawade.github.io/post/matformers/</guid>
      <description>Google recently released the Gemma 3n models — E4B and E2B. The models are packed with novel components and features from PLEs, ASR and AST, MobileNet-V5 for vision etc. But one of the more interesting parts in the announcement is that E2B isn’t just a smaller sibling trained separately (or distilled) like other family of models we have previously seen (7B, 11B, 70B etc) — it’s actually a sub-model within E4B. Even more intriguing, the release mentioned that it’s possible to “mix and match” layers between the two, depending on memory and compute constraints to create even more models of different sizes. How was such a modularity achieved? How are different sized models trained simultaneously?</description>
    </item>
    
    <item>
      <title>ORPO — Preference Optimization without Reference Model</title>
      <link>https://www.bhavinjawade.github.io/post/orpo/</link>
      <pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.bhavinjawade.github.io/post/orpo/</guid>
      <description>Typically, preference alignment in large language models (LLMs) requires a reference model and a warm-up phase of supervised fine-tuning. ORPO proposes a monolithic approach by integrating preference alignment directly into the supervised fine-tuning (SFT) stage, ensuring that the model can learn human preferences during instruction tuning itself.</description>
    </item>
    
    <item>
      <title>Tuning-Free Longer Context Lengths For LLMs — A Review of Self-Extend (LLM Maybe LongLM)</title>
      <link>https://www.bhavinjawade.github.io/post/selfextend/</link>
      <pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.bhavinjawade.github.io/post/selfextend/</guid>
      <description>LLMs are typically trained on fixed-length sequences, leading to performance degradation when dealing with longer texts due to positional Out-Of-Distribution (O.O.D) issues. The paper proposes a solution called &amp;lsquo;Self-Extend,&amp;rsquo; which uses grouped attention to handle longer sequences by mapping out-of-distribution positions into the trained range. This approach combines normal and grouped attention, maintaining precision for nearby tokens and context awareness for distant tokens. The method significantly reduces perplexity in models and improves performance in various NLP tasks without impacting short-context tasks.</description>
    </item>
    
    <item>
      <title>Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training</title>
      <link>https://www.bhavinjawade.github.io/post/gqa/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.bhavinjawade.github.io/post/gqa/</guid>
      <description>The article explores Grouped Query Attention (GQA), an efficient pre-training strategy for large language models (LLMs) like LLaMA-2 and Mistral7B. It describes GQA as a hybrid of multi-head attention (MHA) and multi-query attention (MQA), providing a balance between computational efficiency and model quality. The article also discusses the challenges of MHA, such as memory bandwidth, and how GQA addresses these by grouping query heads to optimize training and inference in large-scale models.</description>
    </item>
    
    <item>
      <title>Understanding LoRA — Low Rank Adaptation For Finetuning Large Models</title>
      <link>https://www.bhavinjawade.github.io/post/lora/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.bhavinjawade.github.io/post/lora/</guid>
      <description>Fine-tuning large pre-trained models is computationally challenging, often involving adjustment of millions of parameters. This traditional fine-tuning approach, while effective, demands substantial computational resources and time, posing a bottleneck for adapting these models to specific tasks. LoRA presented an effective solution to this problem by decomposing the update matrix during finetuing.</description>
    </item>
    
  </channel>
</rss>