[{"authors":["admin"],"categories":null,"content":"I am a Research Scientist at Netflix Research, working on Multimodal Large Language Models, Reward Modeling, and Reasoning. I did my Ph.D. in Computer Science from the University at Buffalo, where I was advised by Dr. Venu Govindaraju. During my Ph.D., I was a part of the Center for Unified Biometrics and Sensors (CUBS) Lab.\nPreviously, I was a research scientist intern at Netflix Research, Yahoo Research, and Adobe Research. My doctoral research primarily focused on Multimodal Contrastive Learning, Deep Metric Learning, and Deep Feature Fusion. My Ph.D. was funded by the IARPA BRIAR, NSF AI Institute, Qualcomm, NSF CITeR, NSF DiBBS, and NSF S\u0026amp;CC programs.\nAmong some notable achievements, I received the IEEE Best Paper Award at IJCB 2023, the Graduate Leadership Award (2023) and the Russell Agrusa Research Innovation Award (2021), as well as the Best CSE Ph.D. Poster and SEAS Ph.D. Poster Awards in 2022.\nNews   Feb 2025: Glad to share that I have joined Netflix Research as full-time Research Scientist  Nov 2025: Glad to share that I our paper at Netflix Research - \"RSMS: Audio-Visual Representation Learning For Lip-Sync Estimation Through Ranking Augmented Contrastive Training\" was accepted at ICASSP 2025  Oct 2024: Glad to share our paper 'ProxyFusion: Face Feature Aggregation Through Sparse Experts' got accepted at NeurIPS 2024  August 2024: Glad to share our paper 'SCOT: Self-Supervised Contrastive Pre-training For Zero-Shot Compositional Retrieval' got accepted at WACV 2025 in Round 1 (12.1% Acceptance Rate)  May 2024: Started summer internship at Netflix Research as a Research Scientist.  Dec 2023: Honored to receive the Graduate Leadership Award, from Department of Computer Science, University at Buffalo  Feb 2024: Glad to share our paper 'GestSpoof: Gesture Based Spatio-Temporal Representation Learning For Robust Fingerprint Presentation Attack Detection.' got accepted at FG 2024  Dec 2023: Glad to share our paper 'Conditional Neural Aggregation Network For Unconstrained Long Range Person Feature Fusion' got accepted in IEEE TBIOM Journal  Sept 2023: CoNAN received the Best Paper Award at IJCB 2023. Link  July 2023: Glad to share our paper title: 'CoNAN - Conditional Neural Aggregation Network for Unconstrained face recognition' has been accepted at IJCB 2023.  May 2023: Started summer internship at Yahoo Research as a Research Scientist.  April 2023: Glad to share our paper 'RealCQA - Scientific Chart Question Answering as a Test-bed for First-Order Logic' got accepted at ICDAR 2023.  April 2023: Presented my work on spatio-temporal fingerprint spoof detection at CITeR-EAB workshop at Idiap, Switzerland.  Feb 2023: Honored as one of the winner of SEAS PhD Research Poster Award 2023, by School of Engineering and Applied Sciences, at University at Buffalo  Feb 2023: I will be co-organizing the IJCB LivDet 2023 Challenge on detecting contactless fingerprint spoofs.  Feb 2023: Our book chapter on Deep Metric Learning in Handbook Of Statistics is now available online.  Jan 2023: Met with SUNY Chancellor Dr. John B King (Former US Secretary of Education) as part of his visit to UB.  Jan 2023: Presented our two papers (NAPReg and Hear The Flow) at WACV 2023 in Hawaii.   Dec 2022: Awarded Best CSE PhD Poster Award 2022 by University at Buffalo  Dec 2022: Presented our work on multi-modal fusion and feature aggregation (CoNAN) at IARPA BRIAR program review  Nov 2022: Presented our work on fingerprint spoof detection through temporal learning at NSF CITeR's Fall 2022 Conference Oct 2022: Glad to share our paper 'NAP Regularization' got accepted at WACV 2023 Oct 2022: Glad to share our paper 'Hear the Flow' got accepted at WACV 2023 Oct 2022: Glad to share our paper on attribute de-biased vision transformers (AD-ViT) got accepted at AVSS 2022 Oct 2022: Presented our paper on RidgeBase dataset at IJCB 2022  Sep 2022: Our dataset RidgeBase is now available for public use Sep 2022: Presented our work on contactless fingerprint recongition at FedID conference  Jun 2022: Won of the winners of Adobe Code Jam 2022 May 2022: Started summer internship at Adobe Research as Research Scientist Dec 2021: Awarded Russell Agrusa Research Innovation Award  Oct 2021: Won Blackstone launchpad Best Idea Award 2021  Aug 2021: Started serving as the President of Computer Science Graduate Student Association  Jun 2021: Glad to share our paper 'MultiLoss Fusion For Contactless Fingerprinting' got accepted at WIFS 2021 Jan 2021: Won Govt. of British Columbia's Maple Ridge Hackathon     News May 2022 - Joined Adobe as a Research Scientist Intern for summer 2022. Dec 2021 - Got honored with the Russell Agrusa Research Innovation Award. Sept 2021 - Recieved Blackstone Launchpad Best Idea Award under social and climate change category Aug 2021 - Started serving as the President for the Computer Science Graduate Student Association at University at Buffalo Feb 2021 - Won the Maple Ridge Hackathon organized Govt. British Comlumbia. -- ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://www.bhavinjawade.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am a Research Scientist at Netflix Research, working on Multimodal Large Language Models, Reward Modeling, and Reasoning. I did my Ph.D. in Computer Science from the University at Buffalo, where I was advised by Dr. Venu Govindaraju. During my Ph.D., I was a part of the Center for Unified Biometrics and Sensors (CUBS) Lab.\nPreviously, I was a research scientist intern at Netflix Research, Yahoo Research, and Adobe Research. My doctoral research primarily focused on Multimodal Contrastive Learning, Deep Metric Learning, and Deep Feature Fusion.","tags":null,"title":"Bhavin Jawade","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536476400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536476400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://www.bhavinjawade.github.io/tutorial/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Tutorials and Docs","type":"docs"},{"authors":["Bhavin Jawade"],"categories":[],"content":"Google recently released the Gemma 3n models — E4B and E2B. The models are packed with novel components and features from PLEs, ASR and AST, MobileNet-V5 for vision etc. But one of the more interesting parts in the announcement is that E2B isn’t just a smaller sibling trained separately (or distilled) like other family of models we have previously seen (7B, 11B, 70B etc) — it’s actually a sub-model within E4B. Even more intriguing, the release mentioned that it’s possible to “mix and match” layers between the two, depending on memory and compute constraints to create even more models of different sizes. How was such a modularity achieved? How are different sized models trained simultaneously?\nIt turns out Gemma 3n uses an architecture called MatFormer — short for Matryoshka Transformer — (DeepMind, NeurIPS 2024). The idea is surprisingly elegant: train a larger model (E4B) in such a way that smaller, fully functional sub-models (like E2B) are embedded inside it. Like nested Matryoshka dolls, except here, it’s transformer layers. No distillation. No retraining. No routing like mixture of experts. Just shared weights and slicing while still preserving effective accuracy.\nIn this post, we’ll go deep into how MatFormer works — what parts of the architecture are shared, how sub-models are extracted, what makes the training objective feasible, and why this changes how we think about model scaling and deployment, especially for constrained environments.\n  Image taken from Gemma 3n Developer Guide   MatFormer — Matryoshka Transformer A standard transformer layer consists of two learnable blocks — projections matrices in the Multi-head self-attention and the Feed-forward network (FFN). Feed-forward networks (FFNs) are the dominant contributors to both the parameter count and inference latency, and Matformer papers presents most of its experiments by applying the proposed nested architecture to FFN block (though it can be applied to any learnable weight).\nIn large-scale language models like Llama 3 and Gemma 3, the Feed-Forward Network (FFN) within each transformer block is typically implemented as a two-layer MLP (Gated MLP):\n First Linear Layer (Expansion/Intermediate): Projects the input embedding to a higher-dimensional hidden space. Activation Function: Often uses GELU, ReLU, or more recently, gated variants like SwiGLU or SiLU. Second Linear Layer (Projection): Projects the hidden representation back to the original embedding dimension    Transformer's FFN feedforward equation   Here $d$ is token embedding dimension (model dimension). $d_{ff}$ is the Feed-forward hidden dimension. It’s the size of the intermediate activation in the FFN (feed-forward network) block. It’s typically much larger than $d$ ($d$ \u0026raquo; $d_{ff}$), usually 4x–6x as large. For example, in a LLaMA-style transformer with $d$ = 4096, FFN often uses $d_{ff}$ = 11008 or more. These layers dominate model size and memory usage.\nNested multiple FFN widths from one FFN: Instead of fixing a single FFN width per layer, MatFormer nests multiple FFNs of different widths inside each transformer block.\nLet $d_{ff}$ be the largest FFN width we want to support (e.g. 16384), then sub-models with FFN width m_i​ simply use the top-left submatrix of the full weight matrices.\nFor example, let’s say $d$ = 4096 (input token dimension) and d_ff = 16384 (full FFN hidden dimension) and we choose a set of granularities we want to train for: m1 = 4096, m2 = 8192, m3 = 12288 and m4 = 16384. Here the idea is that submodules of smaller widths are subsets of those with larger widths.\n  Training All Granularities Jointly The next question is: how do we make sure all sub-models perform well? Rather than jointly computing the loss across all submodels in each training step, MatFormer adopts a more efficient approach: stochastic sampling of submodel granularities during training.\nMatFormer uses a multi-granularity loss, computed as:\n  $M_i$​ is the submodel corresponding to the i-th granularity (i.e., using only the first $m_i$ neurons in each FFN).\nThis design has two critical benefits. Only one submodel is evaluated per step, avoiding the overhead of computing all granularities simultaneously. Additionally, since each submodel shares parameters with the larger model, training a different one in each step still updates the shared weight matrices.\nIn the paper they uniformly sample each submodel with equal probability while computing this loss.\n In the experiments in appendix of the matformer, authors experiment with tuning the sampling probabilities for individual granularities. They find that upweight the loss for the largest granularity gives boost to the performance of bigger models while modest degradation to the performance of the smaller submodels.\n Way more number of accurate smaller models for free (Mix’n’Match) One of the most interesting aspects of MatFormer is that it doesn’t just give you g (number of trained) nested submodels — it gives you hundreds of submodels for free at inference time via a simple technique called Mix’n’Match.\nDuring training, MatFormer explicitly optimizes $g$ submodels, each corresponding to a specific granularity (i.e., FFN width) across all layers. For example, one submodel might use width $m_1$​ (e.g., 4096) in all layers, another might use $m_3$​ (e.g., 12288) everywhere. But this doesn’t restrict us to uniform widths during inference.\nAt inference time, we can vary the FFN width layer by layer — one layer could use $m_2$​, the next $m_3$​, the next $m_2$ again, and so on. Since all the FFN blocks are nested, these hybrid models require no retraining, no fine-tuning, and no architectural redefinition.\nThis layer-wise configuration space is huge — for L layers and g granularities, there are g to the power L possible submodels. Even for small values (e.g., 32 layers, 4 granularities), that’s billions of options.\nBut not all of these perform equally well. To select a good configuration, the authors propose a simple heuristic:\n Prefer submodels where FFN widths vary slowly and consistently across layers.\n In other words: Avoid large jumps between small and large granularities from one layer to the next.\n Also, favor monotonic or gently increasing sequences — e.g., use m_2​ for early layers and gradually grow to m_3​ in later layers.\n this heuristic aligns well with how the model was trained — where sampled submodels always used uniform widths per step. So while hybrid layer-wise combinations were never explicitly trained, configurations that look similar to the training regime generalize better.\n  The authors of Matformer applied this approach to two transformer based models — LLMs and ViTs (called MatLM and MatViT respectively)\nFor LLMs trained with Matformer’s nested FFN, the authors find that all granularity submodels of Matformer outperform their respective separately or independently trained similar sized vanilla transformers LLMs. In above the experiments for MatLM, they trained 4 nested granularities with $d_{ffn}$ (FFN dimension) / $d$ (model dimension) ratios of → {0.5, 1, 2, 4}. They named respective models as S, M, L and XL. Here XL is the full model.\n Even though in the matformer paper the maximum size transformer trained was 850M parameters, Gemma 3n utilizing matformer for 4B (effective 5B) shows parametric scalability of this approach.\n Performance of Mix’n’Match models They also observed that Mix’n’Match models tend to lie on the accuracy-to-compute trade-off curve (Pareto-optimal accuracy-vs-model-size). Which means that based on the available compute resources, the biggest possible Mix’n’Match model can be created during inference, which showed likely have better accuracy then relatively smaller trained submodel. This has interesting implications. For instance, many a times in deployment use-cases we have enough compute resources (GPU memory) to load a 20B parameter model, but the nearest variant that is available to us is either a 14B or 33B. Using the 14B parameter models leads to underutilization of resources and subpar performance and the 33B parameter model won’t fit in memory. This Mix’n’match approach allows to create nearly perfect sized variants for free with commensurate accuracy trade-off.\n Gemma 3n being based on the Matformer architecture also gives us the liberty to perform Mix’n’Match and create many more model sizes between 2B and 4B. In terms of the performance, Gemma 3n E4B has MMLU accuracy of 62.3%, and E2B has 50.90%. Mix’n’Match ‘ed variants such as 2.54B has MMLU accuracy of 55.40% and 2.69B has 57.70%, which significantly better than 2B and little lower than 4B. This allows us to get perfectly sized accurate model for on-device deployment usecase — smartphone, tab, mac, etc.\n To try out creating mix’n’match variants of the Gemma 3n model, Gemma 3n team has released Matformer Lab, which is colab notebook where you can slice the model and push it to huggingface. They have also released some optimal slicing configurations (as we saw above though we can generated g to the power L models, not all mix’n’matches might be optimal for desired size) on hugging face — google/gemma3n-slicing-configs. These configurations include varying the FFN dimension at either layer level, which is more finegrained or at block level which includes 4 local layers and 1 global layer.\n  Image taken from Matformer paper — https://arxiv.org/pdf/2310.07707   Another aspect of training smaller models as nested submodels of the bigger model that authors focus on is consistency of predicted tokens. One way verifying this consistency is by looking at rejection rates of speculative decoding. In speculative decoding, instead of always autoregressively predicting the tokens from the large target model, there is a smaller “Draft model” and a larger “Verifier model” (target model). The draft model performs token speculation, i.e. it predicts the next n tokens along with their probabilities, and the target model parrallely performs verification of the predicted tokens. Comparing the target model’s probabilities for the same tokens predicted by the draft model rejection sampling is performed. When the draft models is significantly off with respect to the target model, the draft model’s predictions are rejected. As one could see, here the most significant speed-ups come when the draft model consistently predicts the similar token distribution as the target model, leading to lesser rejections. If rejections would be more, then we won’t get the desired inference speed ups expected from speculative decoding. Authors of Matformer, show that since the smaller submodels are nested within and are trained along side the larger (XL) model they show more consistent nature required for drafter and verifier models. When traditional speculative decoding using a independently trained “S” sized model with XL model as target led to 10% inference speed time speedup over autoregressive decoding of target model, using the “S” sized MatLM model led to 16% speedup.\nMoreover since the two MatLM models (S and XL) are trained together, it allows for shared attention caches (this cannot be done with indepedently trained models since the latent reps would be very different).\n What I find even more interesting is that during deployment, the universal XL model can be stored in memory and based on the compute constraints be used to extract an adaptable smaller model. This dynamic resource adaptive model extraction and inference might be really useful for edge use-cases.\n The Case for Image Retrieval Authors also trained the Matformer for ViTs (Vision transformers) encoders (MatViT). Apart from similar trends as observed for MatLM, an interesting use-case of MatViT is in fast image retrieval. In typical image retriveal, getting embeddings for gallery images (gallery creation) happens offline and are stored, while quering (creating embedding for the query image) happens real-time. This means that gallery embeddings can utilize bigger more accurate models, while the query embeddings might require a smaller model for fast inference. Since smaller submodels in Matformers are nested and trained jointly, they tend to share the same embedding space as the bigger model and lead to meaningful distances for nearest neighbour based retrieval. This allows for using the XL model for gallery encoding, while using a smaller (ex: S) model for querying.\n  Image taken from the original Matformer paper   How are MatFormers different from MoEs? Almost all major recent open LLM transformers are MoE architectures. Mixture of Experts (MoE) models increase capacity by adding multiple FFNs — called experts — to each transformer layer. At inference time, a router decides which expert(s) to use for each token, and only those selected paths are executed. This reduces compute cost per forward pass, since most experts stay inactive. However, all expert weights still need to be loaded into memory, because the router can choose any of them at runtime. This makes MoE models difficult to deploy on memory-constrained devices — while compute is sparse, memory usage stays high. In contrast, MatFormer doesn’t rely on routing or multiple experts. Instead, each FFN is designed to support multiple granularities by nesting smaller sub-networks inside larger ones. At inference time, a specific submodel can be selected and run independently. Only the parameters for that submodel need to be loaded into memory. This makes MatFormer much better suited for on-device or low-memory inference, where storing the full model isn’t feasible.\nConclusion MatFormer represents a significant shift in how we approach model training, inference, and deployment. Traditionally, large models are trained with massive compute budgets, and smaller variants are created afterward — either trained separately or distilled from the larger ones. MatFormer breaks from this pattern by nesting multiple functional submodels within a single transformer via shared weight training, eliminating the need for costly distillation, retraining, or complex routing like Mixture-of-Experts. This design unlocks a smooth continuum of resource-adaptive models — from lightweight, mobile-ready deployments to full-capacity inference engines — all derived from a single base model. The success of Gemma 3n’s E2B and E4B, the versatility of Mix’n’Match variants, and the cross-modal generalizability demonstrated by MatViT encoders suggest that MatFormer-style architectures could become the new default for how research labs release scalable model families.\nResources  Matformer: Devvrit, Fnu, et al. “Matformer: Nested transformer for elastic inference.” Advances in Neural Information Processing Systems 37 (2024): 140535–140564. https://arxiv.org/abs/2310.07707 Speculative Decoding: Leviathan, Yaniv, Matan Kalman, and Yossi Matias. “Fast inference from transformers via speculative decoding.” ICML, PMLR, 2023 https://arxiv.org/pdf/2211.17192 NeurIPS 2023 video and slides: https://neurips.cc/virtual/2023/80908 Gemma 3n Developer Guide: https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/ Hugging Face Blog: https://huggingface.co/blog/gemma3n MatFormer Lab: [https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Gemma_3n]MatFormer_Lab.ipynb Gemma 3 Technical Report: https://arxiv.org/pdf/2503.19786v1  Footnote on Gemma 3 attention: See Gemma 3 technical paper for details on global and local attention\n   Subscribe at the bottom of this page to get updated when a new article comes up!\n","date":1752364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752364800,"objectID":"fe6630a552cfd19ad1fb3836cbe76bba","permalink":"https://www.bhavinjawade.github.io/post/matformers/","publishdate":"2025-07-13T00:00:00Z","relpermalink":"/post/matformers/","section":"post","summary":"Google recently released the Gemma 3n models — E4B and E2B. The models are packed with novel components and features from PLEs, ASR and AST, MobileNet-V5 for vision etc. But one of the more interesting parts in the announcement is that E2B isn’t just a smaller sibling trained separately (or distilled) like other family of models we have previously seen (7B, 11B, 70B etc) — it’s actually a sub-model within E4B. Even more intriguing, the release mentioned that it’s possible to “mix and match” layers between the two, depending on memory and compute constraints to create even more models of different sizes. How was such a modularity achieved? How are different sized models trained simultaneously?","tags":["Architecture","Machine Learning","Deep Learning","LLM","Transformers","AI"],"title":"Understanding MatFormer - Nested Transformers for elastic inference","type":"post"},{"authors":["Bhavin Jawade"],"categories":[],"content":"Training Large Language Models (LLMs) Training large language models (LLMs) is typically a multi-stage process:\n Pretraining Supervised Finetuning (SFT) Preference Alignment  Pretraining is where it all starts. The model is fed massive amounts of web-scraped textual data using next token prediction objective to learn the fundamentals of language. It picks up on grammar, context, and even some common sense, giving it a broad understanding of how language works. By the end of this stage, the model can generate coherent text across various topics, but it’s still a generalist. To build instruction following agents (or conversational bots) the second stage — Supervised Finetuning (Instruction Finetuning) is performed. Here, the model is trained on specific datasets with task-oriented instructions. This stage refines the model’s ability to follow instructions, making it better at tasks like answering questions or summarizing text. Essentially, it turns the model from a general language generator into a more specialized tool for practical applications.\nHowever, despite the ability to follow instructions, models may generate harmful, undesirable, unethical or not-so benign responses. To solve this perference alignment is performed as an additional step. Preference alignment fine-tunes the model to ensure it aligns with human values and avoids generating harmful or biased content. This is usually done with reinforcement learning from human feedback (RLHF), where the model learns to prefer outputs that human evaluators favor. The goal is to make sure the model not only performs well but does so in a way that aligns with ethical standards and user expectations. This is done on labelled / annotated datasets of preference pairs, where labellers select their preference answer or response for a given query. Apart from RLHF there are other techniques such as DPO, KTO, etc used for preference optimization.\nORPO: A Monolithic Preference Alignment Approach Typically, preference alignment requires a reference model and a warm-up phase of supervised fine-tuning. To eliminate the reliance on the reference model and the multi-stage training process, ORPO proposes a monolithic approach. The authors aim to integrate preference alignment directly into the supervised fine-tuning (SFT) training, ensuring that the model can learn human preferences during the instruction tuning phase itself.\nThe paper (ORPO) emphasizes the importance of SFT in preference alignment, noting that a small penalty for undesired generation styles can be sufficient to guide the model. ORPO leverages this by using a straightforward odds ratio mechanism during SFT, which efficiently contrasts favored and disfavored responses. This method is designed to achieve stable convergence without the complexities and potential instability associated with reinforcement learning methods like PPO.\nORPO’s Approach In traditional SFT, there’s no explicit mechanism to penalize the model for generating disfavored responses. ORPO introduces a method to directly contrast favored (chosen) and disfavored (rejected) responses by calculating the odds ratio.\nOdds of Generating a Sequence Odds of Generating a Sequence — The odds of generating a sequence $y$ given an input $x$ is defined as:\n$$ odds_{\\theta}(y \\mid x) = \\frac{P_{\\theta}(y \\mid x)}{1 - P_{\\theta}(y \\mid x)} $$\nThis helps quantify how much more likely the model is to generate the sequence $y$ compared to not generating it. Using these odds the authors define Odds Ratio (OR). The core idea of ORPO is to use the odds ratio to create a penalty that will encourage the model to favor the chosen response over the rejected one. The odds ratio between a favored response $y_w$ and a disfavored response $y_l$​ is:\n$$ OR_\\theta(y_w, y_l) = \\frac{odds_\\theta(y_w \\mid x)}{odds_\\theta(y_l \\mid x)} $$\nThis ratio directly compares how much more likely the model is to generate the favored response compared to the disfavored one. By optimizing this ratio, the model learns to increase the likelihood of generating preferred responses.\nLoss Function: Combining SFT and Preference Alignment The ORPO objective function combines the standard SFT loss (negative log-likelihood) with a penalty term based on the odds ratio. The SFT loss is the standard negative log-likelihood:\n Standard SFT Loss (negative log-likelihood):  $$ L_{SFT} = -\\log P_{\\theta}(y_w \\mid x) $$\n Odds Ratio-based Penalty:\n$$ L_{OR} = -\\log \\sigma \\left( \\log \\frac{odds_{\\theta}(y_w \\mid x)}{odds_{\\theta}(y_l \\mid x)} \\right) $$\n  The penalty term L_OR is based on the log odds ratio, adjusted by a sigmoid function to smooth the gradient.\nThis term penalizes the model if the odds ratio between the chosen and disfavored responses is not sufficiently large.\nThe overall loss function is weighted sum of the two terms:\n$$ L_{ORPO} = \\mathbb{E}_{(x, y_w, y_l)} \\left[ L_{SFT} + \\lambda \\cdot L_{OR} \\right] $$\nThe objective combines the standard SFT loss with a new term that penalizes the model if it fails to differentiate sufficiently between favored and disfavored responses. The hyperparameter λ controls the strength of this penalty.\nResults The authors performed various experiments on multiple LLMs ranging for 125M parameters to 7B parameter models. This includes Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B). ORPO consistently improved performance, surpassing state-of-the-art models with significantly larger parameters. ORPO-tuned models, such as Mistral-ORPO-α (7B) and Mistral-ORPO-β (7B), achieved up to 12.20% on AlpacaEval2.0. In the MT-Bench multi-turn instruction-following benchmark, Mistral-ORPO-α (7B) and Mistral-ORPO-β (7B) scored 7.23 and 7.32, respectively, which are comparable to or better than a few larger models. They also performed experiments to evaluate diversity of response. ORPO maintained a good balance between per-input diversity (low within-input variation) and across-input diversity (high between-input variation).\nIn this article we will look a new LLM preference optimization approach ORPO: Monolithic Preference Optimization without Reference Model. Do checkout my other articles on LLMs, including Self-Extend, GQA, etc.\nReferences  ORPO: Monolithic Preference Optimization without Reference Model — https://arxiv.org/abs/2403.07691 Training language models to follow instructions with human feedback — https://arxiv.org/abs/2203.02155  ","date":1740009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740009600,"objectID":"53f37207cbefdee7dd8d41c71927919f","permalink":"https://www.bhavinjawade.github.io/post/orpo/","publishdate":"2025-02-20T00:00:00Z","relpermalink":"/post/orpo/","section":"post","summary":"Typically, preference alignment in large language models (LLMs) requires a reference model and a warm-up phase of supervised fine-tuning. ORPO proposes a monolithic approach by integrating preference alignment directly into the supervised fine-tuning (SFT) stage, ensuring that the model can learn human preferences during instruction tuning itself.","tags":["Reward Modeling","Machine Learning","Deep Learning","LLM","Optimization","AI"],"title":"ORPO — Preference Optimization without Reference Model","type":"post"},{"authors":["**Bhavin Jawade**","Alexander Stone","Deen Dayal Mohan","Xiao Wang","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1720076400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720076400,"objectID":"c376b0cc0c3cc16d9a92cf0a8e134d06","permalink":"https://www.bhavinjawade.github.io/publication/proxyfusion/","publishdate":"2024-07-04T00:00:00-07:00","relpermalink":"/publication/proxyfusion/","section":"publication","summary":"(NeurIPS 2024) The Thirty-eighth Annual Conference on Neural Information Processing Systems","tags":[],"title":"ProxyFusion: Face Feature Aggregation Through Sparse Experts","type":"publication"},{"authors":["**Bhavin Jawade**","Joao Soares","Kapil Thadani","Deen Dayal Mohan","Amir Erfan Eshratifar","Benjamin Culpepper","Paloma de Juan","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1720076400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720076400,"objectID":"57a0ee12c3d336b7998b0084f1c92480","permalink":"https://www.bhavinjawade.github.io/publication/scotwacv/","publishdate":"2024-07-04T00:00:00-07:00","relpermalink":"/publication/scotwacv/","section":"publication","summary":"(WACV 2025) IEEE/CVF Winter Conference on Applications of Computer Vision","tags":[],"title":"SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval","type":"publication"},{"authors":["**Bhavin Jawade**","Ravi Teja Gadde","Christophe Bejjani","Yinghong Lan"],"categories":null,"content":"","date":1717484400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717484400,"objectID":"d48a2c9d282d42313b1ec31894c81b8b","permalink":"https://www.bhavinjawade.github.io/publication/netlfix_rsms/","publishdate":"2024-06-04T00:00:00-07:00","relpermalink":"/publication/netlfix_rsms/","section":"publication","summary":"(ICASSP 2025) IEEE International Conference on Acoustics, Speech, and Signal Processing 2025","tags":[],"title":"Audio-Visual Representation Learning For Lip-Sync Estimation Through Ranking Augmented Contrastive Training","type":"publication"},{"authors":["Bhavin Jawade"],"categories":[],"content":"Before we get started\nRead this article on Towards Data Science\nFollow me on medium: https://bhavinjawade.medium.com/\nIn this article we will look at the paper “LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning” by Hongye Jin et. al. that was just released on arxiv 2 days ago (2nd Jan 2023).\nLLMs like GPT-3 or BERT are typically trained on fixed-length sequences. This design choice arises from practical constraints: managing computational resources and maintaining efficiency during training. As a result, these models are usually trained on sequences of a predetermined maximum length (like 512 tokens for BERT, or a few thousand for larger models). The limitation here is that the model’s self-attention mechanism, a key component that helps the model understand and generate language, is trained to only attend to contexts within this fixed length.\nDuring training, the model learns to create representations of input sequences, incorporating both the content (tokens) and position of each token. Positional encoding , a method to incorporate the order of words, plays a crucial role here. However, since the training involves fixed-length sequences, the model’s understanding of positional relationships is confined to this range. When faced with longer sequences at inference time, the model encounters positions and relative distances between tokens that it has never seen before, leading to what the authors refer to as “positional O.O.D (Out-Of-Distribution)” issues. Essentially, the model’s performance degrades because it is processing input that is different in structure from what it was trained on.\nThe challenge, therefore, lies in enabling these models to handle longer sequences then they were trained on, without encountering performance degradation due to positional O.O.D issues. This problem is especially pertinent in real-world applications where input sequences can be much longer than the training sequences, and the ability to maintain context over these longer lengths is crucial for tasks like document summarization, extended conversation understanding, or reading lengthy technical documents. The authors argue that LLMs are naturally equipped to handle longer text sequences than they are typically trained on, the only major bottleneck to this potential is O.O.D positional encodings. The paper suggested a simple yet effective strategy to adapt the longer ‘inference time’ sequences to models trained on limited context length without any finetuning.\nBefore we talk about the paper’s approach lets quickly look at perplexity metric used to evaluate LLMs.\nPerplexity (PPL) Perplexity (PPL) is a commonly used metric in NLP to evaluate the performance of language models. It’s a measure of how well a probabilistic model predicts a sample. In the context of language models, perplexity gauges how well the model predicts a sequence of words. Perplexity is defined as the exponentiated average negative log-likelihood of a sequence of words. Mathematically\n$$ PPL(X) = \\exp \\left( -\\frac{1}{t} \\sum_{i} \\log p(\\theta | x_i, x_{\u0026lt;i}) \\right) $$\nA lower perplexity score indicates a better language model. It means the model assigns higher probabilities, on average, to the test samples it sees. In other words, the model is less ‘surprised’ by the actual sequence of words it encounters, which is why the term ‘perplexity’ is used. Perplexity is usually used for causal models like GPT, Mistral etc (and not for MLMs like BERT). A great blog on huggingface to understand perplexity: https://huggingface.co/docs/transformers/perplexity\nSelf-Extend In standard LLMs, each token in a sequence attends to every other token, considering their relative positions. This positional information is critical for understanding the context and relationships between words. However, this mechanism is based on the maximum sequence length seen during training. When the input sequence exceeds this length, the model encounters positions it has never seen before (O.O.D.), leading to a decrease in performance.\nGrouped Attention: To convert the O.O.D. positions into the range that model is trained to see, a simple mathematical “floor division” (denoted as “//” in programming) operation is performed.\nFor example, consider a sequence of 8 tokens and a group size of 2. The original positions [0, 1, 2, 3, 4, 5, 6, 7] would be mapped to [0, 0, 1, 1, 2, 2, 3, 3]. By grouping tokens, the model can handle sequences longer than its original training limitation, effectively extending its context window.\nBut this inaccuracy in positions where multiple tokens have same positional embedding can leads to some degradataion in performance. The paper shows that with small group size PPL (Perplexity) is a slightly higher than the original LLMs.\nThe neighboring tokens are the most important tokens to generate the next token. For tokens that are close to each other, the precise relative positioning is essential for understanding the immediate context, such as the syntax and semantics of a sentence. Whereas when the tokens are far apart in a text, the precise relative positions between them become less crucial for understanding the overall context or meaning. The exact order of words in distant parts of a text is less important than the general thematic connection between those parts.\nThis is where the paper proposes to combine the grouped attention (which has the FLOOR operation applied) with normal attention (with regular positions). The authors propose using normal attention for a predefined “neighbor window” around each token and grouped attention for tokens outside this window. Using the notations from the paper, ‘L’: Pretraining context window size, ‘G’: Group size for grouped attention, ‘wn’​: Window size for neighbor tokens:\nThe authors introduce a shift in the relative position for grouped attention by wn​ − (wn // G)​​ to ensure a smooth transition between normal and grouped attention areas. Finally, they merge the two parts of attention by replacing the attention values outside the neighbor token window with the values from grouped attention.\nFor example, considering a sequence where the original model’s context window is 7 tokens, the group size is 2, and the neighbor window is 4 tokens. For token positions [0, 1, 2, 3, 4, 5, 6], normal attention is applied to [0, 1, 2, 3] and grouped attention to [4, 5, 6]. The attention matrix will be a blend of the two, with the precise attention for the first four tokens and grouped attention for the last three. The final attention matrix is then used to compute the output of the Transformer’s attention layer, maintaining both local precision and long-range context awareness.\nConclusion The paper showed comparison against LLaMA-2, Mistral and SOLAR with and without Self-Extend. As per the paper, Self-extend significantly decreases the perplexity of the models for long context window sizes. The models were also tested on a variety of tasks such as single-document and multi-document question answering, summarization, and few-shot learning. In most cases, LLMs with Self-Extend outperformed their original versions and even some fine-tuned models on these benchmarks. The models were also tested on various short-context tasks from the Hugging Face Open LLM benchmark suite. There was negligible impact on the performance of these tasks, indicating that Self-Extend does not adversely affect the model’s ability to handle shorter texts.\nThis task asks a language model to find a basic passkey (a random five-digit number) hidden in a lengthy, nonsensical text sequence scattered at various levels. The findings reveal that Self-Extend, without specific adjustments, achieves 100% success in finding the passkey at all tested depths and context lengths.\nAdditionally, the results show that even though Mistral w/ SWA (Sliding Window Attention) has a reduced PPL outside its initial training context range, it’s limited to extracting information (like the passkey) only within its sliding window.\nOverall, this suggests that Self-Extend successfully leverages the inherent capabilities of LLMs for extended contexts.\n","date":1704326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704326400,"objectID":"4f4faea95d0115c0a25d677b6a36ba37","permalink":"https://www.bhavinjawade.github.io/post/selfextend/","publishdate":"2024-01-04T00:00:00Z","relpermalink":"/post/selfextend/","section":"post","summary":"LLMs are typically trained on fixed-length sequences, leading to performance degradation when dealing with longer texts due to positional Out-Of-Distribution (O.O.D) issues. The paper proposes a solution called 'Self-Extend,' which uses grouped attention to handle longer sequences by mapping out-of-distribution positions into the trained range. This approach combines normal and grouped attention, maintaining precision for nearby tokens and context awareness for distant tokens. The method significantly reduces perplexity in models and improves performance in various NLP tasks without impacting short-context tasks.","tags":["Bigdata","Machine Learning","Deep Learning","Opensource","LLM"],"title":"Tuning-Free Longer Context Lengths For LLMs — A Review of Self-Extend (LLM Maybe LongLM)","type":"post"},{"authors":["Bhavin Jawade"],"categories":[],"content":"Before we get started\nRead this article on Towards Data Science\nFollow me on medium: https://bhavinjawade.medium.com/\nIntroduction In the previous article on training large-scale models, we looked at LoRA. In this article, we will examine another strategy adopted by different large language models for efficient training — Grouped Query Attention (GQA). In short, Grouped Query Attention (GQA) is a generalization of multi-head attention (MHA) and multi-query attention (MQA) — with each of them being a special case of GQA. Therefore, before we dive into Grouped Query Attention, let’s revisit traditional multi-head attention proposed by Vaswani et al. in the seminal “Attention is All You Need” paper. Following that, we will explore Multi-query attention and how it addresses challenges with MHA. Finally, we will answer the questions “What is GQA?” and “How does it give us the best of both worlds?”\nMulti-head Attention Multi-head attention is a critical component of Transformer models, enabling them to efficiently process and understand complex sequences in tasks like language translation, summarization, and more. To grasp its intricacies, we must delve into the mathematical underpinnings and understand how multiple heads in the attention mechanism function.\nThe basic attention mechanism computes a weighted sum of values, with weights dependent on a query and a set of keys. Mathematically, this is expressed as:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nThis is referred to as scaled dot product attention. In this equation, Q (Query) and K (Key) are matrices representing the queries and keys. V (Value) is the matrix for values. “d_k” is the dimensionality of keys, which is used for scaling.\nExpanding with Multi-Head Attention (MHA) Multi-head attention employs multiple ‘heads’ of attention layers, enabling the model to attend to information from different representation subspaces. In each head, there is an independent set of linear layers (projection matrices) for the query, key, and values (this is an important point that we will revisit in GQA). For each head (numbered h):\n$$ headʰ = Attention(Q.Wqʰ,K.Wkʰ,V.Wvʰ) $$ shell Copy code\nConcatenating Head Outputs The outputs of the individual heads are concatenated and then linearly transformed.\n$$ MultiHead(Q,K,V) = Concat(head¹,head²,…,headʰ) .Wᵒ $$\nWᵒ is another weight matrix that linearly transforms the concatenated vector to the final output dimension.\nThe intuition behind multi-head attention is that by applying the attention mechanism multiple times in parallel, the model can capture different types of relationships in the data.\nSource: Sections of the diagram from \u0026ldquo;Attention is All You Need\u0026rdquo; paper [https://arxiv.org/abs/1706.03762], composition by the author\nThe Memory Bandwidth Challenge in Multi-Head Attention The crux of the issue lies in the memory overhead. Each decoding step in autoregressive models like Transformers requires loading decoder weights along with all attention keys and values. This process is not only computationally intensive but also memory bandwidth-intensive. As model sizes grow, this overhead also increases, making scaling up an increasingly arduous task.\nEmergence of Multi-Query Attention (MQA) Multi-query attention (MQA) emerged as a solution to mitigate this bottleneck. The idea is simple yet effective: use multiple query heads but only a single key and value head. This approach significantly reduces the memory load, enhancing inference speed. It has been employed in multiple large-scale models such as PaLM, StarCoder, and Falcon.\nIn multi-query attention, we average the heads for keys and values so that all query heads share the same key and value head. This is achieved by replicating the mean-pooled “head” H times, where H is the number of query heads.\nSource: https://arxiv.org/pdf/2305.13245.pdf\nGrouped Query Attention Grouped-query attention (GQA) is a simple approach that blends elements of multi-head attention (MHA) and multi-query attention (MQA) to create a more efficient attention mechanism. The mathematical framework of GQA can be understood as follows:\nDivision into Groups In GQA, the query heads (Q) from a traditional multi-head model are divided into G groups. Each group is assigned a single key (K) and value (V) head. This configuration is denoted as GQA-G, where G represents the number of groups.\nSource: https://arxiv.org/pdf/2305.13245.pdf\nConclusion In this post, we first looked at traditional multi-head attention (MHA) and its variant Multi-query attention. Then we looked at a more generic formulation GQA, which is used by many LLM models for effective pre-training. GQA combines multi-head attention (MHA) with multi-query attention (MQA), providing a fair trade-off between quality and speed. GQA minimizes memory bandwidth demands by grouping query heads, making it appropriate for scaling up models. GQA has been used in place of typical multi-head attention in recent models such as the LLaMA-2 and Mistral7B.\nReferences:  GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints MQA: Fast Transformer Decoding: One Write-Head is All You Need MHA: Attention is all you need  ","date":1702598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702598400,"objectID":"393d8f67a1f8109973518c4ad11d90b6","permalink":"https://www.bhavinjawade.github.io/post/gqa/","publishdate":"2023-12-15T00:00:00Z","relpermalink":"/post/gqa/","section":"post","summary":"The article explores Grouped Query Attention (GQA), an efficient pre-training strategy for large language models (LLMs) like LLaMA-2 and Mistral7B. It describes GQA as a hybrid of multi-head attention (MHA) and multi-query attention (MQA), providing a balance between computational efficiency and model quality. The article also discusses the challenges of MHA, such as memory bandwidth, and how GQA addresses these by grouping query heads to optimize training and inference in large-scale models.","tags":["Bigdata","Machine Learning","Deep Learning","Opensource","LLM"],"title":"Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training","type":"post"},{"authors":["Bhavin Jawade"],"categories":[],"content":"Fine-Tuning Large Pre-Trained Models Fine-tuning large pre-trained models is computationally challenging, often involving adjustment of millions of parameters. This traditional fine-tuning approach, while effective, demands substantial computational resources and time, posing a bottleneck for adapting these models to specific tasks. LoRA presented an effective solution to this problem by decomposing the update matrix during fine-tuning. To study LoRA, let us start by first revisiting traditional fine-tuning.\nDecomposition of (ΔW) In traditional fine-tuning, we modify a pre-trained neural network’s weights to adapt to a new task. This adjustment involves altering the original weight matrix (W) of the network. The changes made to (W) during fine-tuning are collectively represented by (ΔW), such that the updated weights can be expressed as (W + ΔW).\nNow, rather than modifying (W) directly, the LoRA approach seeks to decompose (ΔW). This decomposition is a crucial step in reducing the computational overhead associated with fine-tuning large models.\nThe Intrinsic Rank Hypothesis The intrinsic rank hypothesis suggests that significant changes to the neural network can be captured using a lower-dimensional representation. Essentially, it posits that not all elements of (ΔW) are equally important; instead, a smaller subset of these changes can effectively encapsulate the necessary adjustments.\nIntroducing Matrices (A) and (B) Building on this hypothesis, LoRA proposes representing (ΔW) as the product of two smaller matrices, (A) and (B), with a lower rank. The updated weight matrix (W’) thus becomes:\n[ W\u0026rsquo; = W + BA ]\nIn this equation, (W) remains frozen (i.e., it is not updated during training). The matrices (B) and (A) are of lower dimensionality, with their product (BA) representing a low-rank approximation of (ΔW).\nImpact of Lower Rank on Trainable Parameters By choosing matrices (A) and (B) to have a lower rank (r), the number of trainable parameters is significantly reduced. For example, if (W) is a (d x d) matrix, traditionally, updating (W) would involve (d²) parameters. However, with (B) and (A) of sizes (d x r) and (r x d) respectively, the total number of parameters reduces to (2dr), which is much smaller when (r \u0026laquo; d).\nThe reduction in the number of trainable parameters, as achieved through the Low-Rank Adaptation (LoRA) method, offers several significant benefits, particularly when fine-tuning large-scale neural networks:\n Reduced Memory Footprint: LoRA decreases memory needs by lowering the number of parameters to update, aiding in the management of large-scale models. Faster Training and Adaptation: By simplifying computational demands, LoRA accelerates the training and fine-tuning of large models for new tasks. Feasibility for Smaller Hardware: LoRA’s lower parameter count enables the fine-tuning of substantial models on less powerful hardware, like modest GPUs or CPUs. Scaling to Larger Models: LoRA facilitates the expansion of AI models without a corresponding increase in computational resources, making the management of growing model sizes more practical.  In the context of LoRA, the concept of rank plays a pivotal role in determining the efficiency and effectiveness of the adaptation process. Remarkably, the paper highlights that the rank of the matrices A and B can be astonishingly low, sometimes as low as one.\nAlthough the LoRA paper predominantly showcases experiments within the realm of Natural Language Processing (NLP), the underlying approach of low-rank adaptation holds broad applicability and could be effectively employed in training various types of neural networks across different domains.\nConclusion LoRA’s approach to decomposing (ΔW) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model’s learning capability is preserved with significantly fewer parameters.\nThis article is published in towards data science magzine. Read it there - https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6\nReferences:  Hu, Edward J., et al. “LoRA: Low-rank adaptation of large language models.” arXiv preprint arXiv:2106.09685 (2021).  ","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701907200,"objectID":"a8a9673f1141353fb193f134168f5876","permalink":"https://www.bhavinjawade.github.io/post/lora/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/post/lora/","section":"post","summary":"Fine-tuning large pre-trained models is computationally challenging, often involving adjustment of millions of parameters. This traditional fine-tuning approach, while effective, demands substantial computational resources and time, posing a bottleneck for adapting these models to specific tasks. LoRA presented an effective solution to this problem by decomposing the update matrix during finetuing. ","tags":["Bigdata","Machine Learning","Deep Learning","Opensource","LLM"],"title":"Understanding LoRA — Low Rank Adaptation For Finetuning Large Models","type":"post"},{"authors":["**Bhavin Jawade**","Deen Dayal Mohan","Naji Mohammed","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1701676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701676800,"objectID":"3e3aca4dd6d16383b66aa5db0d31b1b3","permalink":"https://www.bhavinjawade.github.io/publication/naploss/","publishdate":"2023-12-04T00:00:00-08:00","relpermalink":"/publication/naploss/","section":"publication","summary":"(WACV 2023) IEEE/CVF Winter Conference on Applications of Computer Vision","tags":[],"title":"NAPReg: Nouns As Proxies Regularization for Semantically Aware Cross-Modal Embeddings","type":"publication"},{"authors":["**Bhavin Jawade**","Deen Dayal Mohan","Dennis Fedorishin","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1700156841,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700156841,"objectID":"e556392f99faf5840e92c0068b2b6b30","permalink":"https://www.bhavinjawade.github.io/publication/conan/","publishdate":"2023-11-16T09:47:21-08:00","relpermalink":"/publication/conan/","section":"publication","summary":"(IJCB 2023), IEEE International Joint Conference on Biometrics","tags":["Face Recognition","Feature Aggregation","Unconstrained Environments","Machine Learning","Best Paper","Oral Paper","Biometrics"],"title":"CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion *(Best Paper Award)* (Oral)","type":"publication"},{"authors":["**Bhavin Jawade**","Deen Dayal Mohan","Prajwal Shetty","Dennis Fedorishin","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1697474841,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697474841,"objectID":"46cab01249c057b31fac742e160dc842","permalink":"https://www.bhavinjawade.github.io/publication/tbiomconan/","publishdate":"2023-10-16T09:47:21-07:00","relpermalink":"/publication/tbiomconan/","section":"publication","summary":"IEEE Transactions on Biometrics, Behavior, and Identity Science (TBIOM), 2024","tags":["Face Recognition","Feature Aggregation","Unconstrained Environments","Machine Learning","Best Paper","Oral Paper","Biometrics"],"title":"Conditional Neural Aggregation Network For Unconstrained Long Range Biometric Feature Fusion","type":"publication"},{"authors":["Saleem Ahmed","**Bhavin Jawade**","Shubham Pandey","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1692428400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692428400,"objectID":"7546769ad0c632669dd294aaf12103e7","permalink":"https://www.bhavinjawade.github.io/publication/realcqa/","publishdate":"2023-08-19T00:00:00-07:00","relpermalink":"/publication/realcqa/","section":"publication","summary":"IEEE/IAPR International Conference on Document Analysis and Recognition (ICDAR), 2023","tags":["Chart QA","First-Order Logic","Visual Complexity","Neural Networks"],"title":"RealCQA: Scientific Chart Question Answering as a Test-Bed for First-Order Logic","type":"publication"},{"authors":["Dennis Fedroishin","Deen Dayal Mohan","**Bhavin Jawade**","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1675324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675324800,"objectID":"fd6ae4a33aa23a2610e153fbbbb6a8c5","permalink":"https://www.bhavinjawade.github.io/publication/heartheflow/","publishdate":"2023-02-02T00:00:00-08:00","relpermalink":"/publication/heartheflow/","section":"publication","summary":"IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022","tags":[],"title":"Hear The Flow: Optical Flow-Based Self-Supervised Visual Sound Source Localization","type":"publication"},{"authors":["Deen Dayal Mohan","**Bhavin Jawade**","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1672732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672732800,"objectID":"76d3758505a96b66e9e5839c7a2683d9","permalink":"https://www.bhavinjawade.github.io/publication/dml_book_chapter/","publishdate":"2023-01-03T00:00:00-08:00","relpermalink":"/publication/dml_book_chapter/","section":"publication","summary":"Book Chapter - Handbook of Statistics, Edition 48","tags":[],"title":"Deep metric learning for computer vision: A brief overview","type":"publication"},{"authors":["**Bhavin Jawade**","Deen Dayal Mohan","Srirangaraj Setlur","Nalini Ratha","Venu Govindaraju"],"categories":null,"content":"","date":1660546800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660546800,"objectID":"3d5c090160b43acb1b47fb158ac3e0ea","permalink":"https://www.bhavinjawade.github.io/publication/ridgebase/","publishdate":"2022-08-15T00:00:00-07:00","relpermalink":"/publication/ridgebase/","section":"publication","summary":"IAPR/IEEE International Joint Conference on Biometrics (IJCB), 2022","tags":[],"title":"RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset","type":"publication"},{"authors":["Kyung Won Lee","**Bhavin Jawade**","Deen Dayal Mohan","Srirangaraj Setlur","Venu Govindaraju"],"categories":null,"content":"","date":1657868400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657868400,"objectID":"ba6a8f5ff57acf61bc99ccec2c326017","permalink":"https://www.bhavinjawade.github.io/publication/advit/","publishdate":"2022-07-15T00:00:00-07:00","relpermalink":"/publication/advit/","section":"publication","summary":"IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS), 2022","tags":[],"title":"AD-ViT: Attribute De-biased Vision Transformer for Long-Term Person Re-identification","type":"publication"},{"authors":["**Bhavin Jawade**","Akshay Agarwal","Srirangaraj Setlur","Nalini Ratha"],"categories":null,"content":"","date":1639555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639555200,"objectID":"18a148bcb89a19d42eb3bae9d22c2bb8","permalink":"https://www.bhavinjawade.github.io/publication/multiloss/","publishdate":"2021-12-15T00:00:00-08:00","relpermalink":"/publication/multiloss/","section":"publication","summary":"IEEE International Workshop On Forensics and Security (WIFS), 2021","tags":[],"title":"Multi Loss Fusion For Matching Smartphone Captured Contactless Finger Images","type":"publication"},{"authors":["Antoine Miech","Jean-Baptiste Alayrac","Ivan Laptev","Josef Sivic","Andrew Zisserman"],"categories":null,"content":" \n (adsbygoogle = window.adsbygoogle || []).push({});   Click on the Slides button above to go event page.\n  ","date":1636131600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636131600,"objectID":"ced0378a47f214135fe1c211fd8649e8","permalink":"https://www.bhavinjawade.github.io/talk/thinkingfastandslow/","publishdate":"2019-05-05T00:00:00-07:00","relpermalink":"/talk/thinkingfastandslow/","section":"talk","summary":"Lab's weekly paper presentation | CVPR paper presented at regular lab meeting.","tags":["Meetup","DevCIndore","DevC","fdcIndore","pytorch"],"title":"Paper Presentation - Google Deepmind's Thinking fast and slow: Efficient Text-to-Visual Retrieval with Transformers (CVPR)","type":"talk"},{"authors":["Bhavin Jawade"],"categories":[],"content":"Till 2017, the domain of language modelling, sequence to sequence learning, language translation, image captioning where all dominated by RNNs and LSTMS. For example in a image captioning task you would have CNN based encoder and a LSTM based decoder. Though attention was introduced back in 2015, with NMT paper [Bahdanau et al., 2015] to overcome the fixed context vector drawback of traditional encoder-decoder architecture, it was not untill 2017 \u0026ldquo;Attention is all you need\u0026rdquo; paper that its full potential was explored. Transfomer removed the dependence on LSTMs, RNNs and CNNs by solely applying attention mechanism.\nTransformer What makes the transfomer architecture novel and the paper seminal is that it was the first proposed work that relied entirely on self-attention for the sequence to sequence learning task. Just like previous seq2seq learning architectures, that transfomer model is autoregressive at each step, which means that the predicted token is used as an (additional) input for generating the next token.\nThe basic architecture of transfomer can be divided into 2 parts:\n Encoder Decoder  Neither the encoder or decoder in this case is a CNN or RNN, both of them are composed of feed forward networks and multi-head attention blocks, which we will discuss further.\nTransformer model architecture from attention is all you need paper\nEncoder The encoder in the paper consists of N layers each layer having 2 sublayers:\n Multi-Head attention Feed Forward network  Each sublayers gets a residual input (Skip connection) from previous sublayer followed by a Layer Normalization. Layer normalization is different from batch normalization in the sense that input values for all neurons in the same layer are normalized for each data sample, unlike batch norm where input values of the same neuron for all the data in the mini-batch are normalized [J Lei Ba et.al].\nThe encoder takes as in the positionally encoded input embedding. In the paper the authors used a sinusoidal function to define the positional encoding. In language modeling transformers like BERT, GPT-2 etc they typically use fully learnable matrix inplace of the sinusoidal function.\n$$PE_{(pos,2i+1)} = cos(pos/10000^{2i/dmodel})$$\n$$PE_{(pos,2i)} = sin(pos/10000^{2i/dmodel})$$\nMulti-Head Attention The positionally encoded input embedding goes as input to the multi-head attention block. The Attention mechanism maps the key-value and query to the output. Query determines which words (values) to focus on (attend to). Both Key and Value are the sequences of input length. All key, value and query in case of the encoder, comes from previous layer of the encoder. In case of the decoder\u0026rsquo;s second multi-head attention sublayer query comes from the previous decoder layer, and keys an values come from the output of the encoder.\nScaled Dot-Product Attention {Bahdanau 2015} introduced the additive attention and {Luong 2015} introduced the dot-product attention. Transfomer uses the scaled form of the dot product attention. They used the dot product attention as it is much faster to computer with respect to the additive attention.\nThe Query and Keys are multiplied ($QK^T$) and scaled using $\\sqrt{d_k}$ where $d_k$ is the dimension of the keys, values and query vector. Masking is performed on this scaled matrix to ensure that the networks only sees the past values and not the future. Masking is performed by setting all the future values (above the main diagonal) in the matrix to $-\\infty$. Softmax values are computed for the scaled query key products and multiplied with values.\n$QK^T$ vector is scaled by $\\sqrt{d_k}$ to ensure that dot product does not grow very large in magnitude and softmax doesnot have extremely small gradients.\n$$Attention (Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}).V$$\nMulti-Head Attention: The multi-head attention is formed by projecting the query, value, and keys h times using linear layers and then performing the scaled dot-product attention. The outputs from all heads are concatenated and passed through a linear layer. In the paper they use $h = 8$\n \n (adsbygoogle = window.adsbygoogle || []).push({});  ","date":1620259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620259200,"objectID":"1f82a424415ba7e00c7d29713f8b7cb0","permalink":"https://www.bhavinjawade.github.io/post/p1-transformers/","publishdate":"2021-05-06T00:00:00Z","relpermalink":"/post/p1-transformers/","section":"post","summary":"Till 2017, the domain of language modelling, sequence to sequence learning, language translation, image captioning where all dominated by RNNs and LSTMS. For example in a image captioning task you would have CNN based encoder and a LSTM based decoder. Though attention was introduced back in 2015, with NMT paper [Bahdanau et al., 2015] to overcome the fixed context vector drawback of traditional encoder-decoder architecture, it was not untill 2017 \u0026ldquo;Attention is all you need\u0026rdquo; paper that its full potential was explored.","tags":[],"title":"Attention is all you need (Paper Notes)","type":"post"},{"authors":["Bhavin Jawade"],"categories":[],"content":" \u0026ldquo;To acquire and retain today\u0026rsquo;s increasingly empowered customers, companies need to harness the insights in their data to personalize experiences at scale.\u0026rdquo; - Brandon Purcell.\n International Data Corp. (IDC) has estimated worldwide revenue for big data and business analytics (BDA) solutions to reach $260 billion in 2022. The industries making the largest investments in big data and business analytics solutions are banking, manufacturing, professional services, and government.\nThis Portuguese based Data Science Startup is trying to make predictive analysis accessible to everyone using its next gen platform called Preflet.\nPreflet  Preflet is aiming to build a data-driven culture and improve decision making for emerging businesses. This platform will allow businesses to leverage the potential of their large amount of data to make realtime prediction without having to write a single line of code, or having serious expertise in data science or BDA.\nHere are some of the features Preflet will provide:\n1. Automated Machine Learning 2. Database Integration 3. Querying Data 4. Interactive Visualization 5. Modular API 6. Serverless Computing 7. Big Data Warehousing  \n (adsbygoogle = window.adsbygoogle || []).push({});  A key feature of the application is its next gen machine learning algorithm, that automates the tedious task of choosing the right learning algorithm for your data. The algorithm picks the best performing machine learning model that delivers reliable predictions.\nQuick Links Website, Facebook\n \n (adsbygoogle = window.adsbygoogle || []).push({});   ","date":1566259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566259200,"objectID":"65eea446f48889c1139e0c25a3408306","permalink":"https://www.bhavinjawade.github.io/post/preflet/","publishdate":"2019-08-20T00:00:00Z","relpermalink":"/post/preflet/","section":"post","summary":"This Portuguese based Data Science Startup is trying to make predictive analysis accessible to everyone using its next gen platform called Preflet.","tags":[],"title":"Preflet: Get real-time predictive insights","type":"post"},{"authors":["Bhavin Jawade"],"categories":null,"content":" \n (adsbygoogle = window.adsbygoogle || []).push({});   Click on the Slides button above to Register for the event.\n  ","date":1560704400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560704400,"objectID":"46becf3ac4d04144d67b2898f282b0d7","permalink":"https://www.bhavinjawade.github.io/talk/azureforappdevelopers/","publishdate":"2019-04-11T00:00:00-07:00","relpermalink":"/talk/azureforappdevelopers/","section":"talk","summary":"Azure for App Developers","tags":["Meetup","DevCIndore","DevC","fdcIndore","swiftIndia","Azure"],"title":"Azure For App Developers","type":"talk"},{"authors":[],"categories":null,"content":" \n (adsbygoogle = window.adsbygoogle || []).push({});   Click on the Slides button above to go event page.\n  ","date":1557075600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557075600,"objectID":"2be22eecdcc47eb20e0f54155920dd9d","permalink":"https://www.bhavinjawade.github.io/talk/pytorchf8/","publishdate":"2019-05-05T00:00:00-07:00","relpermalink":"/talk/pytorchf8/","section":"talk","summary":"Facebook Developer Cirlce Indore | F8 Indore Meetup | Pytorch: Deep Learning Framework","tags":["Meetup","DevCIndore","DevC","fdcIndore","pytorch"],"title":"Pytorch: Deep Learning Framework | F8 Indore Meetup","type":"talk"},{"authors":[],"categories":null,"content":" \n (adsbygoogle = window.adsbygoogle || []).push({});   Click on the Slides button above to Register for the event.\n  ","date":1555261200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555261200,"objectID":"fc59c4cc62ba897bcff8dd69a8c0b9cc","permalink":"https://www.bhavinjawade.github.io/talk/womenofdevc/","publishdate":"2019-04-11T00:00:00-07:00","relpermalink":"/talk/womenofdevc/","section":"talk","summary":"Facebook Developer Cirlce Indore | Women in Tech.","tags":["Meetup","DevCIndore","DevC","fdcIndore"],"title":"Women of Developer Circle | Facebook Developer Circles Indore","type":"talk"},{"authors":["Bhavin Jawade"],"categories":[],"content":"Linkedin has always been a active contributor to the opensource community. April 4th, LinkedIn announced a new opensource project Avro2TF\nAvro2TF provides a scalable Spark-based mechanism to efficiently convert data into a format that can be readily consumed by TensorFlow.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  As they explain it in Avro2TF official documentation: \u0026ldquo;To effectively support deep learning at LinkedIn, we need to first address the data processing issues. Most of the datasets used by our ML algorithms (e.g., LinkedIn’s large scale personalization engine Photon-ML) are in Avro format. Each record in a Avro dataset is essentially a sparse vector, and can be easily consumed by most of the modern classifiers. However, the format cannot be directly used by TensorFlow \u0026ndash; the leading deep learning package. The main blocker is that the sparse vector is not in the same format as Tensor. We believe that this is not only a LinkedIn problem, many companies have vast amount of ML data in similar sparse vector format, and Tensor format is still relatively new to many companies. Avro2TF bridges this gap by providing scalable Spark based transformation and extensions mechanism to efficiently convert the data into TF records that can be readily consumed by TensorFlow. With this technology, developers can improve their productivity by focusing on model building rather than data conversion.\u0026rdquo;\n \n (adsbygoogle = window.adsbygoogle || []).push({});  Avro2TF bridges the gap between the spark based data formats and tfrecords by providing a simple config for modelers to obtain tensors from existing training data. Tensor data itself is not self-contained. So, to be loaded to TensorFlow, it is required to carry metadata and Avro2TF fills this gap by providing a distributed metadata collection job.  Avro2TF reads raw user input data with any format supported by Spark to generate Avro or TFRecord tensorized training data.\n Avro2TF exposes to users a JSON config to specify the tensors that a modeler wants to use in training. For each tensor, a user should specify two kinds of information:\na. What existing features are used to construct the tensor.\nb. The expected name, dtype, and shape of the tensor.\n   \n (adsbygoogle = window.adsbygoogle || []).push({});  Avro2TF is completely built on Scala. To build Avro2TF from the repository, on your machine one needs to perform a Gradle build. To build Avro2TF, run:\n./gradlew build The jar required to run Avro2TF will be located in ./avro2tf-cli/build/libs/\nBut the best way to get started with Avro2TF is go with the Docker Image: to install and launch the Avro2TF Open Source Docker image.\nIn your terminal, run the following to launch a container for the docker image:\ndocker run -p 8080:8888 --name avro2tf-offcial-tutoriallinkedin/avro2tf-official-tutorial:latest Copy and paste the URL into your browser to play with a Jupyter notebook. (Notice: Remember to change 8888 to 8080.)\n \n (adsbygoogle = window.adsbygoogle || []).push({});  Docker image from Docker Hub: Docker Image.\nThe jupyter notebook is the official tutorial for Avro2TF.\nInside LinkedIn, Avro2TF is an integral part of a system called TensorFlowIn that helps users easily feed data into the TensorFlow modeling process.\nAs mentioned on the official blog post: \u0026ldquo;TensorFlowIn is a deep learning training library that is compatible with TonY, TensorFlow, and Spark. It contains end-to-end training-related utilities and frameworks. The above figure gives a high-level overview of TensorFlowIn. Since large-scale data processing is an important step that is not only critical to many LinkedIn applications, but is also useful to the larger AI community, we decided to open source this engine after receiving positive internal feedback.\u0026rdquo;\n \n (adsbygoogle = window.adsbygoogle || []).push({});  LinkedIn has in past opensourced various other deeplearning tools like: PhotonML- Photon Machine Learning\n Photon ML is a machine learning library based on Apache Spark. It was originally developed by the LinkedIn Machine Learning Algorithms Team. Currently, Photon ML supports training different types of Generalized Linear Models(GLMs) and Generalized Linear Mixed Models(GLMMs/GLMix model): logistic, linear, and Poisson.\n and TonY: framework to natively run deep learning frameworks on Apache Hadoop.\n TonY is a framework to natively run deep learning jobs on Apache Hadoop. It currently supports TensorFlow and PyTorch. TonY enables running either single node or distributed training as a Hadoop application. This native connector, together with other TonY features, aims to run machine learning jobs reliably and flexibly.\n References and Learning Resources:\n Avro2TF Github Repo: https://github.com/linkedin/Avro2TF\n Avro2TF Official Tutorial: Avro2tf tutorial\n Avro2TF Linkedin Blogpost: Click here\n PhotonML: https://github.com/linkedin/photon-ml\n TonY: https://github.com/linkedin/TonY\n   \n (adsbygoogle = window.adsbygoogle || []).push({});   ","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"4d09b8e4c01e612578b3e908a0a98a2f","permalink":"https://www.bhavinjawade.github.io/post/avrotf/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/post/avrotf/","section":"post","summary":"Linkedin has always been a active contributor to the opensource community. April 4th, LinkedIn announced a new opensource project Avro2TF..","tags":["Bigdata","Machine Learning","Deep Learning","Opensource","Linkedin"],"title":"Linkedin opensources Avro2TF","type":"post"},{"authors":[],"categories":null,"content":" \n (adsbygoogle = window.adsbygoogle || []).push({});   Click on the Slides button above to view the built-in slides feature.\n  ","date":1554483600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554483600,"objectID":"9c05107cf2127ec01c2407890ef0d517","permalink":"https://www.bhavinjawade.github.io/talk/sageuni/","publishdate":"2019-04-05T00:00:00-07:00","relpermalink":"/talk/sageuni/","section":"talk","summary":"Getting started with opensource and creating your first VR app","tags":["Meetup","DevCIndore","DevC","fdcIndore"],"title":"SAGE University Indore: Session on OpenSource with Facebook.","type":"talk"},{"authors":["Bhavin Jawade"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  ","date":1549738800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549738800,"objectID":"a4efc3b962c85bfbe9ecf22bbf84652a","permalink":"https://www.bhavinjawade.github.io/talk/reactiist/","publishdate":"2018-11-12T00:00:00-08:00","relpermalink":"/talk/reactiist/","section":"talk","summary":"3 Hours Hands-On Virtual Reality with React 360.","tags":["Meetup","DevCIndore","DevC","fdcIndore"],"title":"Hands-On React 360","type":"talk"},{"authors":["Bhavin Jawade"],"categories":[],"content":"Today every big company you can think of is investing in Blockchain. From Tech giants like Microsoft and IBM to Financial giants like Goldman Sachs, everyone is trying to be a part of this Blockchain revolution. But to a layman, what is blockchain? How is this technology any better than the current system?\n Briefly, Blockchain is the decentralized and distributed collection of records.\n Is that it? From a broad perspective- Yes! That’s it.\nLet’s understand this system with an example.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  Imagine you are playing gambling with your friends but none of you has any cash at the moment. So you all decide to keep a record of how much each person bets, wins or loses. And later based on the record, you would make the payment. But how can you trust a single person to keep the record? He might tamper with it. So you all decide to keep an individual copy of the record. Anytime, someone wins or loses some amount, each of you makes an entry in your record. Finally, at the end of the game, you compare all the records. The one which doesn’t match is the false record.\nThis is what decentralization means. Not letting a single organization or an individual, be an authority over the data. In our current dominant system, we have the banks, as the regulatory authority. But in Blockchain, there is no single authority. Track of every transaction is kept in records called ledgers, and a copy of each ledger is kept by a large number of nodes.\nNow the question is, who maintains the ledger. Is each person or node in the network responsible for maintaining the ledger? Actually, No. The answer to this question depends on the implementation of the blockchain.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  Let’s see the example of cryptocurrencies. In cryptocurrency market, to keep each transaction unique, most blockchain implementations use cryptography. Each Transaction is encrypted using some crypto algorithm like SHA-256 (bitcoin), KECCAK-256 (ethereum) etc. It’s not easy to decrypt these transactions. It requires high computing machines to iterate over a large range of possible answers and find the correct match for the transaction. So whoever owns such a machine can help in validating the ledger and adding to the blockchain. In return, they are paid by the system itself. Any further, this discussion might shift from blockchain to cryptocurrencies. Yes, the thought of blockchain originated for bitcoin, and bitcoin currently is the biggest implementation of the blockchain. But today the usage of blockchain is growing in almost all fields.\nAnd this is why the one line definition,” Blockchain is the decentralized and distributed collection of records “ is very accurate. These records could be anything. They can records of events, medical records, and other record management activities, such as identity management, transaction processing, documenting provenance, or food traceability. They can be recorded safely and uniquely over the blockchain. Variety of applications is one of the biggest reason every big organization is trying to be a part of Blockchain.\nConclusion: This was my first article about the Blockchain. In the subsequent articles, I’ll talk about the technical implementation of the blockchain. How different companies like Microsoft are trying to make blockchain better. And most importantly how we can be a part of this blockchain revolution.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  If you are interested to study more about the whole blockchain revolution, I would suggest this book: Blockchain Revolution: How the Technology Behind Bitcoin Is Changing Money.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"5672bf8d53895dd4345d19f999b70e64","permalink":"https://www.bhavinjawade.github.io/post/blockchain/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/blockchain/","section":"post","summary":"Today every big company you can think of is investing in Blockchain. From Tech giants like Microsoft and IBM to..","tags":[],"title":"Blockchain?/! The Start of a new revolution","type":"post"},{"authors":["Bhavin Jawade"],"categories":null,"content":"In this hands-on tutorial, we will learn how to train your own haar cascade model on Microsoft Azure. To understand Haarcascade I recommend you to read the seminal research paper on Face recognition by Viola Jones.\nApart from just using the prebuild haar cascade files, In this tutorial, I will teach you how to train your own model on Microsoft Azure to create your own Haarcascade files for object detection.\nBefore we start we will need to create an Azure Virtual Machine. Creating an Azure Virtual Machine is really simple and quick.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  How to create your Azure Virtual Machine. 1. First login to your Microsoft Azure Account: 2. Select Virtual Machines from the left panel and then click on +Add 3. Fill out the details. In the machine size selected fill: 4. Leave the disks and networking sections to defaults and create the virtual machine. 5. Go to the Dashboard and select the new virtual machine you created. 6. Click on Start. Your Virtual Machine will get started. 7. Click on Connect: to see your SSH login credentials: 8. Start the terminal on your PC. type: Getting your Azure VM ready for training:  \n (adsbygoogle = window.adsbygoogle || []).push({});  SSH via your Azure virtual machine credentials. Voila! You are in. Your VM is now ready.\nFrom here I will show you how to create your very own Haar Cascades, so you can track any object you want. Due to the nature and complexity of this task, this tutorial will be a bit longer than usual, but the reward is massive.\nOnce you are inside your VM’s terminal via SSH\ncd ~ sudo apt-get update sudo apt-get upgrade  First, let’s make ourselves a nice workspace directory\nmkdir opencv_workspace cd opencv_workspace  Now that we’re in here, let’s grab OpenCV\nsudo apt-get install git git clone [https://github.com/Itseez/opencv.git](https://github.com/Itseez/opencv.git)  We’ve cloned the latest version of OpenCV here. Now let’s get some essentials:\nCompiler\nsudo apt-get install build-essential  Libraries\nsudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev  Python bindings and such\nsudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev  Finally, let’s grab the OpenCV development library\nsudo apt-get install libopencv-dev   \n (adsbygoogle = window.adsbygoogle || []).push({});  To build a Haar Cascade, you need “positive” images, and “negative” images. The “positive” images are images that contain the object you want to find. This can either be images that just mainly have the object, or it can be images that contain the object, and you specify the ROI (region of interest) where the object is. With these positives, we build a vector file that is basically all of these positives put together. One nice thing about the positives is that you can actually just have one image of the object you wish to detect, and then have a few thousand negative images. Yes, a few thousand. The negative images can be anything, except they cannot contain your object.\nFrom here, with your single positive image, you can use the opencv_createsamples command to actually create a bunch of positive examples, using your negative images. Your positive image will be superimposed on these negatives, and it will be angled and all sorts of things. It actually can work pretty well, especially if you are really just looking for one specific object. If you are looking to identify all screwdrivers, however, you will want to have thousands of unique images of screwdrivers, rather than using the opencv_createsamples to generate samples for you. We\u0026rsquo;ll keep it simple and just use one positive image, and then create a bunch of samples with our negatives.\nIn my case, I am using the center of Rs 100 Note as the positive Image as I want to detect that via my Object detector.\nthe centre of Rs 100 note\nA little-enlarged Image:\nOk great, getting a positive image is no problem! There is just one problem. We need thousands of negative images. Possibly in the future, we may want thousands of positive images too. Where in the world can we do that? There’s quite a useful site, based on the concept of WordNet, called ImageNet. From here, you can find images of just about anything. So how might we get negatives? The whole point of ImageNet is for image training, so their images are pretty specific. Thus, if we search for people, cars, boats, planes…whatever, chances are, there will be not any Rs100 notes. So, let’s find some bulk image URL links. I found the sports/athletics link to have a reported 1,888 images, but you will find a lot of these are totally broken. Let’s find one more: People.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  Scrapping Images on Azure VM (It’s Damn Fast!) Now we will need to scrap a few thousand images from ImageNet to our Azure Virtual Machine. We will write a Python code to do this. And then run this python code on our VM.\nDownload-Images.py\nimport urllib.request import cv2 import numpy as np import os def store_raw_images(): neg_images_link = '//image-net.org/api/text/imagenet.synset.geturls?wnid=n00523513' neg_image_urls = urllib.request.urlopen(neg_images_link).read().decode() pic_num = 1 if not os.path.exists('neg'): os.makedirs('neg') for i in neg_image_urls.split('\\n'): try: print(i) urllib.request.urlretrieve(i, \u0026quot;neg/\u0026quot;+str(pic_num)+\u0026quot;.jpg\u0026quot;) img = cv2.imread(\u0026quot;neg/\u0026quot;+str(pic_num)+\u0026quot;.jpg\u0026quot;,cv2.IMREAD_GRAYSCALE) # should be larger than samples / pos pic (so we can place our image on it) resized_image = cv2.resize(img, (100, 100)) cv2.imwrite(\u0026quot;neg/\u0026quot;+str(pic_num)+\u0026quot;.jpg\u0026quot;,resized_image) pic_num += 1 except Exception as e: print(str(e))  Use SCP via your terminal to send the python file to your VM’s directory.\nNow the best part about using Azure for training is the super high internet speed. These few thousand images will be scrapped in a couple of minutes on your VM because your Azure VM has a very fast internet connection.\n… To be continued.\n","date":1542700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544688000,"objectID":"a754cdbbfceb11169aebc2ff71529b87","permalink":"https://www.bhavinjawade.github.io/post/haarcascade/","publishdate":"2018-11-20T00:00:00-08:00","relpermalink":"/post/haarcascade/","section":"post","summary":"Using Viola Jones algorithm to detect objects in realtime","tags":["Academic","cMachine Learning"],"title":"Training HaarCascade Model on Microsoft Azure.","type":"post"},{"authors":["**Bhavin Jawade**","Khushbu Goyal"],"categories":null,"content":"","date":1542268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542268800,"objectID":"d86fb5df5ffceb879b5b1409d8d0d53b","permalink":"https://www.bhavinjawade.github.io/publication/geofencing/","publishdate":"2018-11-15T00:00:00-08:00","relpermalink":"/publication/geofencing/","section":"publication","summary":"IEEE International Conference on Inventive Computation Technologies (ICICT), 2018","tags":[],"title":"Low Computation in-device geofencing Algorithm using hierarchy based searching for offline usage","type":"publication"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n   Mentor Mentored participating teams during the hackathon. Speaker Spoke on Facebook\u0026rsquo;s Open Source Technologies. Social Media and Images Click here.  ","date":1541901600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541901600,"objectID":"5f348c38000d644ba77a03ac259be3e8","permalink":"https://www.bhavinjawade.github.io/talk/nitbhopal/","publishdate":"2018-11-12T00:00:00-08:00","relpermalink":"/talk/nitbhopal/","section":"talk","summary":"I was invited as a tech speaker and mentor for Version Beta hackathon held at NIT Bhopal.","tags":["Meetup","DevCIndore","DevC","fdcIndore","hackathon"],"title":"Facebook Open Source - React, Reason and Pytorch","type":"talk"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Topic covered\n  Technical Interview Tips\n  Social Media and Images Click here.\n  ","date":1541901600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541901600,"objectID":"80ae90e6d583ce378db60879db4f9c64","permalink":"https://www.bhavinjawade.github.io/talk/nitmlh/","publishdate":"2018-11-12T00:00:00-08:00","relpermalink":"/talk/nitmlh/","section":"talk","summary":"MLH Localhost talk on how to perform well in a coding interview.","tags":[],"title":"Hack the Coding Interview: Algorithm Practice","type":"talk"},{"authors":null,"categories":null,"content":"","date":1540623600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540623600,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://www.bhavinjawade.github.io/project/external-project/","publishdate":"2018-10-27T00:00:00-07:00","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"Now You See Me","type":"project"},{"authors":["Bhavin Jawade"],"categories":null,"content":"September Read: The monk who sold his Ferrari, by Robin Sharma. Its a self help book written in the form of fable. The story is about a millionaire playboy, high profile litigator named \u0026ldquo;Julian Mantle\u0026rdquo;, who has a hectic schedule, crazy lifestyle and priorities that revolve around money and prestige. The story starts with a heart attack that Julian has in the court after which he sells all his levish possessions including his RED FERRARI and goes for a journey to India. In the remote h\n","date":1538204400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569740400,"objectID":"4c6a2bd8bbe58a4f83524dbc85a7ba20","permalink":"https://www.bhavinjawade.github.io/post/book_monk_who_sold_his_ferrari/","publishdate":"2018-09-29T00:00:00-07:00","relpermalink":"/post/book_monk_who_sold_his_ferrari/","section":"post","summary":"A book that will make you rethink about each and aspect of your life. The book is full of zen techniques to enrich your daily life and inspire you to live with discipline.","tags":["BooksReview","Self_Help_Book","SeptemberRead"],"title":"Book Review: The Monk Who Sold His Ferrai","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536476400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536476400,"objectID":"6aaa0a409f1ec91a41d278c6fa7c0952","permalink":"https://www.bhavinjawade.github.io/tutorial/example/example/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/tutorial/example/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Bhavin Jawade"],"categories":null,"content":"Zeros and ones. This is how we imagined computing till now. This is what classical computing is. But a whole new concept is now changing the way we think about computing.\nIn classical computers everything revolves around transistors, the most basic unit of storage. As we know, transistors are a basically a switch, being either on or off and hence representing a binary one or zero. Typical scale for transitors today is around 14 nanometers which is about 500 times less than a human Red blood cell. As this scale is decreasing we are approaching the limits at which classical computation can properly work. The reason being classical physics doesn’t work well in the quantum relm. A notable example of this is the quantum tunneling, in which electrons tunnels (passes) through a barrier that it classically cannot surmount.\nBut scientists are using these unusual properties to their advantage, by building quantum computers.\n A classical computation is like a solo voice — one line of pure tones succeeding each other. A quantum computation is like a symphony — many lines of tones interfering with one another. ― Seth Lloyd, Programming the Universe: A Quantum Computer Scientist Takes on the Cosmos\n As classical computers are driven by bits, Quantum computers are driven by qubits. These qubits have various properties, like superposition and entanglement.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  Superposition: Imagine a coin flip. We know that the result of a coin flip is binary, heads or tails. But we also know that a coin can stay in an undecided state. While its in air, its state is not defined yet, or we can also say that its in both the states. This principle of having both states at the same time, defines the most important property of quantum bits called superposition. Most of us have already heard about superposition in the classical Scrodengier’s Cat story, where a cat is both dead and alive. The moment you try to measure a qubit, it collapses to one of the definite states. If we have 4 classical bits we can have 2 ^4 = 16 possible combinations of values, out of which you use any one combination at a time. But 4 qubits however can be in all those 16 combinations at the same time.\nEntanglement: Two Qubits can be bind to one another in such a way that a change in one’s state makes the other one react instataneously. This means using a one entangled qubit, we can deduce properties of the other one.\nJust like classical bits are manipulated using logic gates, Qubits are manipulated using quantum gates. Quantum gates taking as input the superposition and rotating probabilites give another superposition as output finally collapsing these superpositions to actual sequence of zeros and ones. This allows a complete set of calculations to be done at the same time.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  Quantum computers require much less iterations as compared to classical computers, though you can’t do much with present day quantum computers. A traditional database searching which may require N iterations, with Quantum computers require just square-root N iterations. Interest in the field increased tremendously after Peter Shor’s very surprising discovery of efficient quantum algorithms for the problems of integer factorization and discrete logarithms in 1994. Since most of current classical cryptography assumes that these two problems are computationally hard, the ability to actually build and use a quantum computer would allow us to break most current classical cryptographic systems.\nAlmost every top company is researching on quantum computers. Google’s Bristlecone, a new quantum computing chip with 72 quantum bits, or qubits brings the race for quantum supremecy closer to its end, overtaking the previous record holder IBM with 50 qubit quantum computer. Quantum supremacy or “quantum advantage” is the potential ability of quantum computing devices to solve problems that classical computers practically cannot.\nNow comes the important question, how can I get started with writing quantum algorithms. Microsoft answers this question with its quantum development kit. Microsoft’s quantum development kit comes with a quantum-focused domain specific language called Q#, a quantum simulation environment, and many open source libraries and beginner samples.\n \n (adsbygoogle = window.adsbygoogle || []).push({});  To run a typical quantum algorithm on a simulated environment requiring about 30 qubits, one needs a computer with a RAM of about 16 GB, and this number grows exponentially. As you go from 30 qubits to 40 qubits, this number increases from 16GB to about 16TB. To help with this microsoft offers a quantum simulator hosted on azure.\nYou can write your first quantum algorithm today. To get started just install microsoft quantum development kit on your system https://marketplace.visualstudio.com/items?itemName=quantum.quantum-devkit-vscode.\nYou can use it with visual studio community or with visual studio code. Now clone this repository https://github.com/microsoft/quantum, and try out the samples. To run these samples, you just need to have quantum-devkit and .NET SDK.\nMessage Teleportation code running on my system. Transfering message between 2 qubits\n \n (adsbygoogle = window.adsbygoogle || []).push({});  Some good reads for quantum computing.\nLecture Notes by Ronald de Wolf: https://homepages.cwi.nl/~rdewolf/qcnotes.pdf\nLearning Q# and understanding microsoft quantum development kit:Click here\nQuantum computing by Jozef Gruska: Click here\n","date":1528527600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547366400,"objectID":"61274d16c0ce320039cf4fd35a66d294","permalink":"https://www.bhavinjawade.github.io/post/quantum/","publishdate":"2018-06-09T00:00:00-07:00","relpermalink":"/post/quantum/","section":"post","summary":"Using Viola Jones algorithm to detect objects in realtime","tags":["Academic"],"title":"Quantum Computing and Microsoft Q#","type":"post"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://www.bhavinjawade.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":[],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"https://www.bhavinjawade.github.io/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating.","tags":null,"title":"Slides","type":"slides"}]