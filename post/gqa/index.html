<!DOCTYPE html>
<html lang="en-us">
<head>

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="google-site-verification" content="8Fk09hZ5X7EG1sQBo-CO8Z22Z4WT5XbyAn9ywvu6-nU" />
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-3400148121105734",
      enable_page_level_ads: true
    });
  </script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-3400148121105734",
          enable_page_level_ads: true
    });
  </script>

  <meta name="generator" content="Source Themes Academic 4.1.0">
  <meta name="generator" content="Hugo 0.69.2" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Bhavin Jawade">

  
  
  
    
  
  <meta name="description" content="The article explores Grouped Query Attention (GQA), an efficient pre-training strategy for large language models (LLMs) like LLaMA-2 and Mistral7B. It describes GQA as a hybrid of multi-head attention (MHA) and multi-query attention (MQA), providing a balance between computational efficiency and model quality. The article also discusses the challenges of MHA, such as memory bandwidth, and how GQA addresses these by grouping query heads to optimize training and inference in large-scale models.">

  
  <link rel="alternate" hreflang="en-us" href="https://www.bhavinjawade.github.io/post/gqa/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-136674527-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://www.bhavinjawade.github.io/index.xml" type="application/rss+xml" title="Bhavin Jawade">
  <link rel="feed" href="https://www.bhavinjawade.github.io/index.xml" type="application/rss+xml" title="Bhavin Jawade">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://www.bhavinjawade.github.io/post/gqa/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bhavin Jawade">
  <meta property="og:url" content="https://www.bhavinjawade.github.io/post/gqa/">
  <meta property="og:title" content="Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training | Bhavin Jawade">
  <meta property="og:description" content="The article explores Grouped Query Attention (GQA), an efficient pre-training strategy for large language models (LLMs) like LLaMA-2 and Mistral7B. It describes GQA as a hybrid of multi-head attention (MHA) and multi-query attention (MQA), providing a balance between computational efficiency and model quality. The article also discusses the challenges of MHA, such as memory bandwidth, and how GQA addresses these by grouping query heads to optimize training and inference in large-scale models."><meta property="og:image" content="https://www.bhavinjawade.github.io/post/gqa/featured.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2023-12-15T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2023-12-15T00:00:00&#43;00:00">
  

  

  

  <title>Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training | Bhavin Jawade</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Bhavin Jawade</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
            
          
        

        <li class="nav-item">
          <a class="nav-link" href="https://www.github.com/bhavinjawade" target="_blank" rel="noopener">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/tutorial/">
            
            <span>Tutorials</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/author/admin/resume_nov2025.pdf">
            
            <span>Resume</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  













<div class="article-header d-xl-none">
  <div class="featured-image" style="background-image: url('/post/gqa/featured_hua0f9df5b5705f0e87ce8b50ad1bdbeff_920748_800x0_resize_lanczos_2.png');"></div>
  
</div>


<div class="container-fluid split-header d-none d-xl-block">
  <div class="row">
    <div class="col-6">
      <div class="split-header-content">
        <h1 itemprop="name">Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training</h1>

        
        <p class="page-subtitle">The variant of multi-head attention powering LLMs like LLaMA-2, Mistral7B, etc.</p>
        

        



<meta content="2023-12-15 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2023-12-15 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      
        <a href="/authors/admin/">Bhavin Jawade</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Dec 15, 2023</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/gqa/#disqus_thread"></a>
  

  

  

</div>


        







  










        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Demystifying%20GQA%20%e2%80%94%20Grouped%20Query%20Attention%20for%20Efficient%20LLM%20Pre-training&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f&amp;title=Demystifying%20GQA%20%e2%80%94%20Grouped%20Query%20Attention%20for%20Efficient%20LLM%20Pre-training"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f&amp;title=Demystifying%20GQA%20%e2%80%94%20Grouped%20Query%20Attention%20for%20Efficient%20LLM%20Pre-training"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Demystifying%20GQA%20%e2%80%94%20Grouped%20Query%20Attention%20for%20Efficient%20LLM%20Pre-training&amp;body=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </div>
    </div>
    <div class="col-6">
      <div class="split-header-image">
        <img src="/post/gqa/featured_hua0f9df5b5705f0e87ce8b50ad1bdbeff_920748_680x500_fill_q90_lanczos_smart1_2.png" itemprop="image" alt="">
        
      </div>
    </div>
  </div>
</div>

<div class="article-container d-xl-none">
  <h1 itemprop="name">Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training</h1>

  
  <p class="page-subtitle">The variant of multi-head attention powering LLMs like LLaMA-2, Mistral7B, etc.</p>
  

  



<meta content="2023-12-15 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2023-12-15 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      
        <a href="/authors/admin/">Bhavin Jawade</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Dec 15, 2023</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/gqa/#disqus_thread"></a>
  

  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Demystifying%20GQA%20%e2%80%94%20Grouped%20Query%20Attention%20for%20Efficient%20LLM%20Pre-training&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f&amp;title=Demystifying%20GQA%20%e2%80%94%20Grouped%20Query%20Attention%20for%20Efficient%20LLM%20Pre-training"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f&amp;title=Demystifying%20GQA%20%e2%80%94%20Grouped%20Query%20Attention%20for%20Efficient%20LLM%20Pre-training"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Demystifying%20GQA%20%e2%80%94%20Grouped%20Query%20Attention%20for%20Efficient%20LLM%20Pre-training&amp;body=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fgqa%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

  







  









</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <p><strong>Before we get started</strong><br>
<strong>Read this article on</strong> <a href="https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a">Towards Data Science</a><br>
<strong>Follow me on medium:</strong> <a href="https://bhavinjawade.medium.com/">https://bhavinjawade.medium.com/</a></p>
<h2 id="introduction">Introduction</h2>
<p>In the previous article on training large-scale models, we looked at LoRA. In this article, we will examine another strategy adopted by different large language models for efficient training — Grouped Query Attention (GQA). In short, Grouped Query Attention (GQA) is a generalization of multi-head attention (MHA) and multi-query attention (MQA) — with each of them being a special case of GQA. Therefore, before we dive into Grouped Query Attention, let’s revisit traditional multi-head attention proposed by Vaswani et al. in the seminal “Attention is All You Need” paper. Following that, we will explore Multi-query attention and how it addresses challenges with MHA. Finally, we will answer the questions “What is GQA?” and “How does it give us the best of both worlds?”</p>
<h2 id="multi-head-attention">Multi-head Attention</h2>
<p>Multi-head attention is a critical component of Transformer models, enabling them to efficiently process and understand complex sequences in tasks like language translation, summarization, and more. To grasp its intricacies, we must delve into the mathematical underpinnings and understand how multiple heads in the attention mechanism function.</p>
<p>The basic attention mechanism computes a weighted sum of values, with weights dependent on a query and a set of keys. Mathematically, this is expressed as:</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<p>This is referred to as scaled dot product attention. In this equation, Q (Query) and K (Key) are matrices representing the queries and keys. V (Value) is the matrix for values. “d_k” is the dimensionality of keys, which is used for scaling.</p>
<h3 id="expanding-with-multi-head-attention-mha">Expanding with Multi-Head Attention (MHA)</h3>
<p>Multi-head attention employs multiple ‘heads’ of attention layers, enabling the model to attend to information from different representation subspaces. In each head, there is an independent set of linear layers (projection matrices) for the query, key, and values (this is an important point that we will revisit in GQA). For each head (numbered h):</p>
<p>$$
headʰ = Attention(Q.Wqʰ,K.Wkʰ,V.Wvʰ)
$$
shell
Copy code</p>
<h3 id="concatenating-head-outputs">Concatenating Head Outputs</h3>
<p>The outputs of the individual heads are concatenated and then linearly transformed.</p>
<p>$$
MultiHead(Q,K,V) = Concat(head¹,head²,…,headʰ) .Wᵒ
$$</p>
<p>Wᵒ is another weight matrix that linearly transforms the concatenated vector to the final output dimension.</p>
<p>The intuition behind multi-head attention is that by applying the attention mechanism multiple times in parallel, the model can capture different types of relationships in the data.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*OJVVcxb9QV1WZwcQFA5Sew.png" alt="Illustration of Multi-Head Attention">
<em>Source: Sections of the diagram from &ldquo;Attention is All You Need&rdquo; paper [https://arxiv.org/abs/1706.03762], composition by the author</em></p>
<h3 id="the-memory-bandwidth-challenge-in-multi-head-attention">The Memory Bandwidth Challenge in Multi-Head Attention</h3>
<p>The crux of the issue lies in the memory overhead. Each decoding step in autoregressive models like Transformers requires loading decoder weights along with all attention keys and values. This process is not only computationally intensive but also memory bandwidth-intensive. As model sizes grow, this overhead also increases, making scaling up an increasingly arduous task.</p>
<h3 id="emergence-of-multi-query-attention-mqa">Emergence of Multi-Query Attention (MQA)</h3>
<p>Multi-query attention (MQA) emerged as a solution to mitigate this bottleneck. The idea is simple yet effective: use multiple query heads but only a single key and value head. This approach significantly reduces the memory load, enhancing inference speed. It has been employed in multiple large-scale models such as PaLM, StarCoder, and Falcon.</p>
<p>In multi-query attention, we average the heads for keys and values so that all query heads share the same key and value head. This is achieved by replicating the mean-pooled “head” H times, where H is the number of query heads.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*HaRjh9kaK2h92iASlHTeRw.png" alt="Multi-Query Attention">
<em>Source: <a href="https://arxiv.org/pdf/2305.13245.pdf">https://arxiv.org/pdf/2305.13245.pdf</a></em></p>
<h2 id="grouped-query-attention">Grouped Query Attention</h2>
<p>Grouped-query attention (GQA) is a simple approach that blends elements of multi-head attention (MHA) and multi-query attention (MQA) to create a more efficient attention mechanism. The mathematical framework of GQA can be understood as follows:</p>
<h3 id="division-into-groups">Division into Groups</h3>
<p>In GQA, the query heads (Q) from a traditional multi-head model are divided into G groups. Each group is assigned a single key (K) and value (V) head. This configuration is denoted as GQA-G, where G represents the number of groups.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*XRDph-5WtcnhLpjpEqL2Ow.png" alt="Difference between MHA, GQA, and MQA">
<em>Source: <a href="https://arxiv.org/pdf/2305.13245.pdf">https://arxiv.org/pdf/2305.13245.pdf</a></em></p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post, we first looked at traditional multi-head attention (MHA) and its variant Multi-query attention. Then we looked at a more generic formulation GQA, which is used by many LLM models for effective pre-training. GQA combines multi-head attention (MHA) with multi-query attention (MQA), providing a fair trade-off between quality and speed. GQA minimizes memory bandwidth demands by grouping query heads, making it appropriate for scaling up models. GQA has been used in place of typical multi-head attention in recent models such as the LLaMA-2 and Mistral7B.</p>
<h3 id="references">References:</h3>
<ol>
<li><a href="https://arxiv.org/pdf/2305.13245.pdf">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a></li>
<li><a href="https://arxiv.org/abs/1911.02150">MQA: Fast Transformer Decoding: One Write-Head is All You Need</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">MHA: Attention is all you need</a></li>
</ol>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/bigdata/">Bigdata</a>
  
  <a class="badge badge-light" href="">Machine Learning</a>
  
  <a class="badge badge-light" href="">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/opensource/">Opensource</a>
  
  <a class="badge badge-light" href="/tags/llm/">LLM</a>
  
</div>




    
      






  
  
    
  
  







<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  
  <img class="portrait mr-3" src="/author/admin/avatar_11_huf1e191aac5c1b707a6496a879e870233_107867_250x250_fill_lanczos_center_2.png" itemprop="image" alt="Avatar">
  

  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/authors/admin">Bhavin Jawade</a></h5>
    <h6 class="card-subtitle">Research Scientist @ Netflix</h6>
    <p class="card-text" itemprop="description">My interests include Computer Vision, Location Based Services (LBS) and Application Development</p>
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="/#contact" >
          <i class="fas fa-envelope"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://twitter.com/bhavinjawade" target="_blank" rel="noopener">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://threads.net/bhavinjawade" target="_blank" rel="noopener">
          <i class="fab fa-threads"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://www.linkedin.com/in/bhavinjawade" target="_blank" rel="noopener">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/bhavinjawade" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://bhavinjawade.medium.com/" target="_blank" rel="noopener">
          <i class="fab fa-medium"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://bhavinjawade.substack.com/" target="_blank" rel="noopener">
          <i class="fas fa-rss"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/lora/">Understanding LoRA — Low Rank Adaptation For Finetuning Large Models</a></li>
          
          <li><a href="/post/avrotf/">Linkedin opensources Avro2TF</a></li>
          
          <li><a href="/publication/conan/">CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion *(Best Paper Award)* (Oral)</a></li>
          
          <li><a href="/publication/tbiomconan/">Conditional Neural Aggregation Network For Unconstrained Long Range Biometric Feature Fusion</a></li>
          
        </ul>
      </div>
      
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "bhavinjawade-me" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <form style="text-align: center;" id="formid" method="GET" enctype="application/x-www-form-urlencoded" action = "https://docs.google.com/forms/d/e/1FAIpQLScIS1hIVJyfo1iprQSLmZfKTj9zugkv7X2Ce6kOBaT1F4KChQ/formResponse">
        <input type = "hidden" name="entry.1639441032" id="firstname" value = "User"/>
        <input type = "hidden" name="entry.1590194600" id="lastname" value = ""/>
        <input name="entry.525161483" required placeholder="email address" id="email" type="email" style="width: 60%;outline: none;font-size: 18px;padding: 1%;font-weight: 500;color: #6d6d6d;border-radius: 29px;padding-left: 3%;padding-right: 3%;border: none;border: 1px solid #e2e2e2;border-top-right-radius: 0;border-bottom-right-radius: 0;"/>
        <input type="submit" name = "submit" value="Subscribe" style="margin-left: -5px;background-color: #58baff;border: none;border: 1px solid #58baf8;border-radius: 20px;padding: 1%;padding-left: 3%;padding-right: 3%;border-radius: 29px;font-size: 18px;color: white;cursor: pointer;border-top-left-radius: 0;border-bottom-left-radius: 0;"/>
    </form>
    <script src="http://code.jquery.com/jquery-1.9.1.js"></script>
    <script src="http://malsup.github.com/jquery.form.js"></script> 
    <script type='text/javascript'>
         
        $(document).ready(function() { 
                
                $('#formid').ajaxForm(function() { 
                    alert("Thank you for your comment!"); 
                });
            }); 
    </script>
  <br>
  <p class="powered-by">
    

    Copyright
    <a href="https://www.linkedin.com/in/bhavinjawade" target="_blank" rel="noopener">Bhavin Jawade</a> | 2018-2019 |  
    <a href="mailto:bhavinjawade@gmail.com" target="_blank" rel="noopener">bhavinjawade@gmail.com</a>.
    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
  
<script type="text/javascript">
    var vglnk = {key: '42643b2b96638249d923f00eda66c144'};
    (function(d, t) {
        var s = d.createElement(t);
            s.type = 'text/javascript';
            s.async = true;
            s.src = '//cdn.viglink.com/api/vglnk.js';
        var r = d.getElementsByTagName(t)[0];
            r.parentNode.insertBefore(s, r);
    }(document, 'script'));
</script>

</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//bhavinjawade-me.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.1846cc5c27f3006b56814095ec02281f.js"></script>

  </body>
</html>

