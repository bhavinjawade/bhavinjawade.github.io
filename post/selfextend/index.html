<!DOCTYPE html>
<html lang="en-us">
<head>

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"/>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="google-site-verification" content="8Fk09hZ5X7EG1sQBo-CO8Z22Z4WT5XbyAn9ywvu6-nU" />
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-3400148121105734",
      enable_page_level_ads: true
    });
  </script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-3400148121105734",
          enable_page_level_ads: true
    });
  </script>

  <meta name="generator" content="Source Themes Academic 4.1.0">
  <meta name="generator" content="Hugo 0.69.2" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Bhavin Jawade">

  
  
  
    
  
  <meta name="description" content="LLMs are typically trained on fixed-length sequences, leading to performance degradation when dealing with longer texts due to positional Out-Of-Distribution (O.O.D) issues. The paper proposes a solution called &#39;Self-Extend,&#39; which uses grouped attention to handle longer sequences by mapping out-of-distribution positions into the trained range. This approach combines normal and grouped attention, maintaining precision for nearby tokens and context awareness for distant tokens. The method significantly reduces perplexity in models and improves performance in various NLP tasks without impacting short-context tasks.">

  
  <link rel="alternate" hreflang="en-us" href="https://www.bhavinjawade.github.io/post/selfextend/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-136674527-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://www.bhavinjawade.github.io/index.xml" type="application/rss+xml" title="Bhavin Jawade">
  <link rel="feed" href="https://www.bhavinjawade.github.io/index.xml" type="application/rss+xml" title="Bhavin Jawade">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://www.bhavinjawade.github.io/post/selfextend/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bhavin Jawade">
  <meta property="og:url" content="https://www.bhavinjawade.github.io/post/selfextend/">
  <meta property="og:title" content="Tuning-Free Longer Context Lengths For LLMs — A Review of Self-Extend (LLM Maybe LongLM) | Bhavin Jawade">
  <meta property="og:description" content="LLMs are typically trained on fixed-length sequences, leading to performance degradation when dealing with longer texts due to positional Out-Of-Distribution (O.O.D) issues. The paper proposes a solution called &#39;Self-Extend,&#39; which uses grouped attention to handle longer sequences by mapping out-of-distribution positions into the trained range. This approach combines normal and grouped attention, maintaining precision for nearby tokens and context awareness for distant tokens. The method significantly reduces perplexity in models and improves performance in various NLP tasks without impacting short-context tasks."><meta property="og:image" content="https://www.bhavinjawade.github.io/post/selfextend/featured.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2024-01-04T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2024-01-04T00:00:00&#43;00:00">
  

  

  

  <title>Tuning-Free Longer Context Lengths For LLMs — A Review of Self-Extend (LLM Maybe LongLM) | Bhavin Jawade</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Bhavin Jawade</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
            
          
        

        <li class="nav-item">
          <a class="nav-link" href="https://www.github.com/bhavinjawade" target="_blank" rel="noopener">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/tutorial/">
            
            <span>Tutorials</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/author/admin/bhavinjawade_cv_feb2025.pdf">
            
            <span>Resume</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  













<div class="article-header d-xl-none">
  <div class="featured-image" style="background-image: url('/post/selfextend/featured_hu2c40791d7f8c6a038ff6d5e67056f657_494664_800x0_resize_lanczos_2.png');"></div>
  
</div>


<div class="container-fluid split-header d-none d-xl-block">
  <div class="row">
    <div class="col-6">
      <div class="split-header-content">
        <h1 itemprop="name">Tuning-Free Longer Context Lengths For LLMs — A Review of Self-Extend (LLM Maybe LongLM)</h1>

        
        <p class="page-subtitle">A simple strategy to enable LLMs to consume longer context length inputs during inference without the need for finetuning.</p>
        

        



<meta content="2024-01-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2024-01-04 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      
        <a href="/authors/admin/">Bhavin Jawade</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jan 4, 2024</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/selfextend/#disqus_thread"></a>
  

  

  

</div>


        







  










        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Tuning-Free%20Longer%20Context%20Lengths%20For%20LLMs%20%e2%80%94%20A%20Review%20of%20Self-Extend%20%28LLM%20Maybe%20LongLM%29&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f&amp;title=Tuning-Free%20Longer%20Context%20Lengths%20For%20LLMs%20%e2%80%94%20A%20Review%20of%20Self-Extend%20%28LLM%20Maybe%20LongLM%29"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f&amp;title=Tuning-Free%20Longer%20Context%20Lengths%20For%20LLMs%20%e2%80%94%20A%20Review%20of%20Self-Extend%20%28LLM%20Maybe%20LongLM%29"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Tuning-Free%20Longer%20Context%20Lengths%20For%20LLMs%20%e2%80%94%20A%20Review%20of%20Self-Extend%20%28LLM%20Maybe%20LongLM%29&amp;body=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </div>
    </div>
    <div class="col-6">
      <div class="split-header-image">
        <img src="/post/selfextend/featured_hu2c40791d7f8c6a038ff6d5e67056f657_494664_680x500_fill_q90_lanczos_smart1_2.png" itemprop="image" alt="">
        
      </div>
    </div>
  </div>
</div>

<div class="article-container d-xl-none">
  <h1 itemprop="name">Tuning-Free Longer Context Lengths For LLMs — A Review of Self-Extend (LLM Maybe LongLM)</h1>

  
  <p class="page-subtitle">A simple strategy to enable LLMs to consume longer context length inputs during inference without the need for finetuning.</p>
  

  



<meta content="2024-01-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2024-01-04 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      
        <a href="/authors/admin/">Bhavin Jawade</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jan 4, 2024</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/selfextend/#disqus_thread"></a>
  

  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Tuning-Free%20Longer%20Context%20Lengths%20For%20LLMs%20%e2%80%94%20A%20Review%20of%20Self-Extend%20%28LLM%20Maybe%20LongLM%29&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f&amp;title=Tuning-Free%20Longer%20Context%20Lengths%20For%20LLMs%20%e2%80%94%20A%20Review%20of%20Self-Extend%20%28LLM%20Maybe%20LongLM%29"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f&amp;title=Tuning-Free%20Longer%20Context%20Lengths%20For%20LLMs%20%e2%80%94%20A%20Review%20of%20Self-Extend%20%28LLM%20Maybe%20LongLM%29"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Tuning-Free%20Longer%20Context%20Lengths%20For%20LLMs%20%e2%80%94%20A%20Review%20of%20Self-Extend%20%28LLM%20Maybe%20LongLM%29&amp;body=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2fselfextend%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

  







  









</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <p><strong>Before we get started</strong><br>
<strong>Read this article on</strong> <a href="https://medium.com/@bhavinjawade/tuning-free-longer-context-lengths-for-llms-a-review-of-self-extend-llm-maybe-longlm-4bbd9b1021bf">Towards Data Science</a><br>
<strong>Follow me on medium:</strong> <a href="https://bhavinjawade.medium.com/">https://bhavinjawade.medium.com/</a></p>
<p>In this article we will look at the paper “LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning” by Hongye Jin et. al. that was just released on arxiv 2 days ago (2nd Jan 2023).</p>
<p>LLMs like GPT-3 or BERT are typically trained on fixed-length sequences. This design choice arises from practical constraints: managing computational resources and maintaining efficiency during training. As a result, these models are usually trained on sequences of a predetermined maximum length (like 512 tokens for BERT, or a few thousand for larger models). The limitation here is that the model’s self-attention mechanism, a key component that helps the model understand and generate language, is trained to only attend to contexts within this fixed length.</p>
<p>During training, the model learns to create representations of input sequences, incorporating both the content (tokens) and position of each token. Positional encoding , a method to incorporate the order of words, plays a crucial role here. However, since the training involves fixed-length sequences, the model’s understanding of positional relationships is confined to this range. When faced with longer sequences at inference time, the model encounters positions and relative distances between tokens that it has never seen before, leading to what the authors refer to as “positional O.O.D (Out-Of-Distribution)” issues. Essentially, the model’s performance degrades because it is processing input that is different in structure from what it was trained on.</p>
<p>The challenge, therefore, lies in enabling these models to handle longer sequences then they were trained on, without encountering performance degradation due to positional O.O.D issues. This problem is especially pertinent in real-world applications where input sequences can be much longer than the training sequences, and the ability to maintain context over these longer lengths is crucial for tasks like document summarization, extended conversation understanding, or reading lengthy technical documents. The authors argue that LLMs are naturally equipped to handle longer text sequences than they are typically trained on, the only major bottleneck to this potential is O.O.D positional encodings.
The paper suggested a simple yet effective strategy to adapt the longer ‘inference time’ sequences to models trained on limited context length without any finetuning.</p>
<p>Before we talk about the paper’s approach lets quickly look at perplexity metric used to evaluate LLMs.</p>
<h2 id="perplexity-ppl">Perplexity (PPL)</h2>
<p>Perplexity (PPL) is a commonly used metric in NLP to evaluate the performance of language models. It’s a measure of how well a probabilistic model predicts a sample. In the context of language models, perplexity gauges how well the model predicts a sequence of words. Perplexity is defined as the exponentiated average negative log-likelihood of a sequence of words. Mathematically</p>
<p>$$
PPL(X) = \exp \left( -\frac{1}{t} \sum_{i} \log p(\theta | x_i, x_{&lt;i}) \right)
$$</p>
<p>A lower perplexity score indicates a better language model. It means the model assigns higher probabilities, on average, to the test samples it sees. In other words, the model is less ‘surprised’ by the actual sequence of words it encounters, which is why the term ‘perplexity’ is used. Perplexity is usually used for causal models like GPT, Mistral etc (and not for MLMs like BERT). A great blog on huggingface to understand perplexity: <a href="https://huggingface.co/docs/transformers/perplexity">https://huggingface.co/docs/transformers/perplexity</a></p>
<h2 id="self-extend">Self-Extend</h2>
<p>In standard LLMs, each token in a sequence attends to every other token, considering their relative positions. This positional information is critical for understanding the context and relationships between words. However, this mechanism is based on the maximum sequence length seen during training. When the input sequence exceeds this length, the model encounters positions it has never seen before (O.O.D.), leading to a decrease in performance.</p>
<h3 id="grouped-attention">Grouped Attention:</h3>
<p>To convert the O.O.D. positions into the range that model is trained to see, a simple mathematical “floor division” (denoted as “//” in programming) operation is performed.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*PKXzSuqS0IgQuomChqyO7A.png" alt="Souce: https://arxiv.org/pdf/2401.01325.pdf"></p>
<p>For example, consider a sequence of 8 tokens and a group size of 2. The original positions [0, 1, 2, 3, 4, 5, 6, 7] would be mapped to [0, 0, 1, 1, 2, 2, 3, 3]. By grouping tokens, the model can handle sequences longer than its original training limitation, effectively extending its context window.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*l-zD_WdHnKVQmQ17P18AZQ.png" alt="Souce: https://arxiv.org/pdf/2401.01325.pdf"></p>
<p>But this inaccuracy in positions where multiple tokens have same positional embedding can leads to some degradataion in performance. The paper shows that with small group size PPL (Perplexity) is a slightly higher than the original LLMs.</p>
<p>The neighboring tokens are the most important tokens to generate the next token. For tokens that are close to each other, the precise relative positioning is essential for understanding the immediate context, such as the syntax and semantics of a sentence. Whereas when the tokens are far apart in a text, the precise relative positions between them become less crucial for understanding the overall context or meaning. The exact order of words in distant parts of a text is less important than the general thematic connection between those parts.</p>
<p>This is where the paper proposes to combine the grouped attention (which has the FLOOR operation applied) with normal attention (with regular positions). The authors propose using normal attention for a predefined “neighbor window” around each token and grouped attention for tokens outside this window. Using the notations from the paper, ‘L’: Pretraining context window size, ‘G’: Group size for grouped attention, ‘wn’​: Window size for neighbor tokens:</p>
<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*60he-9Ecb7Bio7f2SYnnzA.png" alt="Source: https://arxiv.org/pdf/2401.01325.pdf"></p>
<p>The authors introduce a shift in the relative position for grouped attention by wn​ − (wn // G)​​ to ensure a smooth transition between normal and grouped attention areas. Finally, they merge the two parts of attention by replacing the attention values outside the neighbor token window with the values from grouped attention.</p>
<p>For example, considering a sequence where the original model’s context window is 7 tokens, the group size is 2, and the neighbor window is 4 tokens. For token positions [0, 1, 2, 3, 4, 5, 6], normal attention is applied to [0, 1, 2, 3] and grouped attention to [4, 5, 6]. The attention matrix will be a blend of the two, with the precise attention for the first four tokens and grouped attention for the last three. The final attention matrix is then used to compute the output of the Transformer’s attention layer, maintaining both local precision and long-range context awareness.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The paper showed comparison against LLaMA-2, Mistral and SOLAR with and without Self-Extend. As per the paper, Self-extend significantly decreases the perplexity of the models for long context window sizes. The models were also tested on a variety of tasks such as single-document and multi-document question answering, summarization, and few-shot learning. In most cases, LLMs with Self-Extend outperformed their original versions and even some fine-tuned models on these benchmarks. The models were also tested on various short-context tasks from the Hugging Face Open LLM benchmark suite. There was negligible impact on the performance of these tasks, indicating that Self-Extend does not adversely affect the model’s ability to handle shorter texts.</p>
<p>This task asks a language model to find a basic passkey (a random five-digit number) hidden in a lengthy, nonsensical text sequence scattered at various levels. The findings reveal that Self-Extend, without specific adjustments, achieves 100% success in finding the passkey at all tested depths and context lengths.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*60he-9Ecb7Bio7f2SYnnzA.png" alt="Source: https://arxiv.org/pdf/2401.01325.pdf"></p>
<p>Additionally, the results show that even though Mistral w/ SWA (Sliding Window Attention) has a reduced PPL outside its initial training context range, it’s limited to extracting information (like the passkey) only within its sliding window.</p>
<p>Overall, this suggests that Self-Extend successfully leverages the inherent capabilities of LLMs for extended contexts.</p>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/bigdata/">Bigdata</a>
  
  <a class="badge badge-light" href="">Machine Learning</a>
  
  <a class="badge badge-light" href="">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/opensource/">Opensource</a>
  
  <a class="badge badge-light" href="/tags/llm/">LLM</a>
  
</div>




    
      






  
  
    
  
  







<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  
  <img class="portrait mr-3" src="/author/admin/avatar_11_huf1e191aac5c1b707a6496a879e870233_107867_250x250_fill_lanczos_center_2.png" itemprop="image" alt="Avatar">
  

  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/authors/admin">Bhavin Jawade</a></h5>
    <h6 class="card-subtitle">Research Scientist @ Netflix</h6>
    <p class="card-text" itemprop="description">My interests include Computer Vision, Location Based Services (LBS) and Application Development</p>
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="/#contact" >
          <i class="fas fa-envelope"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://twitter.com/bhavinjawade" target="_blank" rel="noopener">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://threads.net/bhavinjawade" target="_blank" rel="noopener">
          <i class="fab fa-threads"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://www.linkedin.com/in/bhavinjawade" target="_blank" rel="noopener">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/bhavinjawade" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://bhavinjawade.medium.com/" target="_blank" rel="noopener">
          <i class="fab fa-medium"></i>
        </a>
      </li>
      
      
      
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="/author/admin/bhavinjawade_cv_feb2025.pdf" >
          <i class="ai ai-cv"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/gqa/">Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training</a></li>
          
          <li><a href="/post/lora/">Understanding LoRA — Low Rank Adaptation For Finetuning Large Models</a></li>
          
          <li><a href="/post/avrotf/">Linkedin opensources Avro2TF</a></li>
          
          <li><a href="/publication/conan/">CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion *(Best Paper Award)* (Oral)</a></li>
          
          <li><a href="/publication/tbiomconan/">Conditional Neural Aggregation Network For Unconstrained Long Range Biometric Feature Fusion</a></li>
          
        </ul>
      </div>
      
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "bhavinjawade-me" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <form style="text-align: center;" id="formid" method="GET" enctype="application/x-www-form-urlencoded" action = "https://docs.google.com/forms/d/e/1FAIpQLScIS1hIVJyfo1iprQSLmZfKTj9zugkv7X2Ce6kOBaT1F4KChQ/formResponse">
        <input type = "hidden" name="entry.1639441032" id="firstname" value = "User"/>
        <input type = "hidden" name="entry.1590194600" id="lastname" value = ""/>
        <input name="entry.525161483" required placeholder="email address" id="email" type="email" style="width: 60%;outline: none;font-size: 18px;padding: 1%;font-weight: 500;color: #6d6d6d;border-radius: 29px;padding-left: 3%;padding-right: 3%;border: none;border: 1px solid #e2e2e2;border-top-right-radius: 0;border-bottom-right-radius: 0;"/>
        <input type="submit" name = "submit" value="Subscribe" style="margin-left: -5px;background-color: #58baff;border: none;border: 1px solid #58baf8;border-radius: 20px;padding: 1%;padding-left: 3%;padding-right: 3%;border-radius: 29px;font-size: 18px;color: white;cursor: pointer;border-top-left-radius: 0;border-bottom-left-radius: 0;"/>
    </form>
    <script src="http://code.jquery.com/jquery-1.9.1.js"></script>
    <script src="http://malsup.github.com/jquery.form.js"></script> 
    <script type='text/javascript'>
         
        $(document).ready(function() { 
                
                $('#formid').ajaxForm(function() { 
                    alert("Thank you for your comment!"); 
                });
            }); 
    </script>
  <br>
  <p class="powered-by">
    

    Copyright
    <a href="https://www.linkedin.com/in/bhavinjawade" target="_blank" rel="noopener">Bhavin Jawade</a> | 2018-2019 |  
    <a href="mailto:bhavinjawade@gmail.com" target="_blank" rel="noopener">bhavinjawade@gmail.com</a>.
    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
  
<script type="text/javascript">
    var vglnk = {key: '42643b2b96638249d923f00eda66c144'};
    (function(d, t) {
        var s = d.createElement(t);
            s.type = 'text/javascript';
            s.async = true;
            s.src = '//cdn.viglink.com/api/vglnk.js';
        var r = d.getElementsByTagName(t)[0];
            r.parentNode.insertBefore(s, r);
    }(document, 'script'));
</script>

</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//bhavinjawade-me.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.1846cc5c27f3006b56814095ec02281f.js"></script>

  </body>
</html>

