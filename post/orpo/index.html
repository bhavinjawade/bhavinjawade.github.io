<!DOCTYPE html>
<html lang="en-us">
<head>

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="google-site-verification" content="8Fk09hZ5X7EG1sQBo-CO8Z22Z4WT5XbyAn9ywvu6-nU" />
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-3400148121105734",
      enable_page_level_ads: true
    });
  </script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-3400148121105734",
          enable_page_level_ads: true
    });
  </script>

  <meta name="generator" content="Source Themes Academic 4.1.0">
  <meta name="generator" content="Hugo 0.69.2" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Bhavin Jawade">

  
  
  
    
  
  <meta name="description" content="Typically, preference alignment in large language models (LLMs) requires a reference model and a warm-up phase of supervised fine-tuning. ORPO proposes a monolithic approach by integrating preference alignment directly into the supervised fine-tuning (SFT) stage, ensuring that the model can learn human preferences during instruction tuning itself.">

  
  <link rel="alternate" hreflang="en-us" href="https://www.bhavinjawade.github.io/post/orpo/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-136674527-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://www.bhavinjawade.github.io/index.xml" type="application/rss+xml" title="Bhavin Jawade">
  <link rel="feed" href="https://www.bhavinjawade.github.io/index.xml" type="application/rss+xml" title="Bhavin Jawade">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://www.bhavinjawade.github.io/post/orpo/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bhavin Jawade">
  <meta property="og:url" content="https://www.bhavinjawade.github.io/post/orpo/">
  <meta property="og:title" content="ORPO — Preference Optimization without Reference Model | Bhavin Jawade">
  <meta property="og:description" content="Typically, preference alignment in large language models (LLMs) requires a reference model and a warm-up phase of supervised fine-tuning. ORPO proposes a monolithic approach by integrating preference alignment directly into the supervised fine-tuning (SFT) stage, ensuring that the model can learn human preferences during instruction tuning itself."><meta property="og:image" content="https://www.bhavinjawade.github.io/post/orpo/featured.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2025-02-20T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2025-02-20T00:00:00&#43;00:00">
  

  

  

  <title>ORPO — Preference Optimization without Reference Model | Bhavin Jawade</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Bhavin Jawade</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
            
          
        

        <li class="nav-item">
          <a class="nav-link" href="https://www.github.com/bhavinjawade" target="_blank" rel="noopener">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/tutorial/">
            
            <span>Tutorials</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/author/admin/bhavinjawade_cv_feb2025.pdf">
            
            <span>Resume</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  













<div class="article-header d-xl-none">
  <div class="featured-image" style="background-image: url('/post/orpo/featured_hu2359322f62a23e61954a3647e8d8a989_463516_800x0_resize_lanczos_2.png');"></div>
  
</div>


<div class="container-fluid split-header d-none d-xl-block">
  <div class="row">
    <div class="col-6">
      <div class="split-header-content">
        <h1 itemprop="name">ORPO — Preference Optimization without Reference Model</h1>

        
        <p class="page-subtitle">Combining instruction fine-tuning and preference alignment in a single stage</p>
        

        



<meta content="2025-02-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2025-02-20 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Bhavin Jawade</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Feb 20, 2025</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/orpo/#disqus_thread"></a>
  

  

  

</div>


        







  










        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=ORPO%20%e2%80%94%20Preference%20Optimization%20without%20Reference%20Model&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f&amp;title=ORPO%20%e2%80%94%20Preference%20Optimization%20without%20Reference%20Model"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f&amp;title=ORPO%20%e2%80%94%20Preference%20Optimization%20without%20Reference%20Model"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=ORPO%20%e2%80%94%20Preference%20Optimization%20without%20Reference%20Model&amp;body=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </div>
    </div>
    <div class="col-6">
      <div class="split-header-image">
        <img src="/post/orpo/featured_hu2359322f62a23e61954a3647e8d8a989_463516_680x500_fill_q90_lanczos_smart1_2.png" itemprop="image" alt="">
        
      </div>
    </div>
  </div>
</div>

<div class="article-container d-xl-none">
  <h1 itemprop="name">ORPO — Preference Optimization without Reference Model</h1>

  
  <p class="page-subtitle">Combining instruction fine-tuning and preference alignment in a single stage</p>
  

  



<meta content="2025-02-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2025-02-20 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Bhavin Jawade</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Feb 20, 2025</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/orpo/#disqus_thread"></a>
  

  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=ORPO%20%e2%80%94%20Preference%20Optimization%20without%20Reference%20Model&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f&amp;title=ORPO%20%e2%80%94%20Preference%20Optimization%20without%20Reference%20Model"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f&amp;title=ORPO%20%e2%80%94%20Preference%20Optimization%20without%20Reference%20Model"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=ORPO%20%e2%80%94%20Preference%20Optimization%20without%20Reference%20Model&amp;body=https%3a%2f%2fwww.bhavinjawade.github.io%2fpost%2forpo%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

  







  









</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <h2 id="training-large-language-models-llms">Training Large Language Models (LLMs)</h2>
<p>Training large language models (LLMs) is typically a multi-stage process:</p>
<ol>
<li><strong>Pretraining</strong></li>
<li><strong>Supervised Finetuning (SFT)</strong></li>
<li><strong>Preference Alignment</strong></li>
</ol>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2YrQtdsjW97TakYh.png" alt="Three stages of LLM Training (i) Pretraining (ii) Supervised Finetuning and (iii) RLHF/preference optimization. Source —"></p>
<p>Pretraining is where it all starts. The model is fed massive amounts of web-scraped textual data using next token prediction objective to learn the fundamentals of language. It picks up on grammar, context, and even some common sense, giving it a broad understanding of how language works. By the end of this stage, the model can generate coherent text across various topics, but it’s still a generalist. To build instruction following agents (or conversational bots) the second stage — Supervised Finetuning (Instruction Finetuning) is performed. Here, the model is trained on specific datasets with task-oriented instructions. This stage refines the model’s ability to follow instructions, making it better at tasks like answering questions or summarizing text. Essentially, it turns the model from a general language generator into a more specialized tool for practical applications.</p>
<p>However, despite the ability to follow instructions, models may generate harmful, undesirable, unethical or not-so benign responses. To solve this perference alignment is performed as an additional step. Preference alignment fine-tunes the model to ensure it aligns with human values and avoids generating harmful or biased content. This is usually done with reinforcement learning from human feedback (RLHF), where the model learns to prefer outputs that human evaluators favor. The goal is to make sure the model not only performs well but does so in a way that aligns with ethical standards and user expectations. This is done on labelled / annotated datasets of preference pairs, where labellers select their preference answer or response for a given query. Apart from RLHF there are other techniques such as DPO, KTO, etc used for preference optimization.</p>
<h2 id="orpo-a-monolithic-preference-alignment-approach">ORPO: A Monolithic Preference Alignment Approach</h2>
<p>Typically, preference alignment requires a reference model and a warm-up phase of supervised fine-tuning. To eliminate the reliance on the reference model and the multi-stage training process, ORPO proposes a monolithic approach. The authors aim to integrate preference alignment directly into the supervised fine-tuning (SFT) training, ensuring that the model can learn human preferences during the instruction tuning phase itself.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9gRtCpAB-PD9G2vR" alt="Typical SFT + RLHF Process"></p>
<p>The paper (ORPO) emphasizes the importance of SFT in preference alignment, noting that a small penalty for undesired generation styles can be sufficient to guide the model. ORPO leverages this by using a straightforward odds ratio mechanism during SFT, which efficiently contrasts favored and disfavored responses. This method is designed to achieve stable convergence without the complexities and potential instability associated with reinforcement learning methods like PPO.</p>
<h3 id="orpos-approach">ORPO’s Approach</h3>
<p>In traditional SFT, there’s no explicit mechanism to penalize the model for generating disfavored responses. ORPO introduces a method to directly contrast favored (chosen) and disfavored (rejected) responses by calculating the odds ratio.</p>
<h4 id="odds-of-generating-a-sequence"><strong>Odds of Generating a Sequence</strong></h4>
<p>Odds of Generating a Sequence — The odds of generating a sequence $y$ given an input $x$ is defined as:</p>
<p>$$
odds_{\theta}(y \mid x) = \frac{P_{\theta}(y \mid x)}{1 - P_{\theta}(y \mid x)}
$$</p>
<p>This helps quantify how much more likely the model is to generate the sequence $y$ compared to not generating it. Using these odds the authors define Odds Ratio (OR). The core idea of ORPO is to use the odds ratio to create a penalty that will encourage the model to favor the chosen response over the rejected one. The odds ratio between a favored response $y_w$ and a disfavored response $y_l$​ is:</p>
<p>$$
OR_\theta(y_w, y_l) = \frac{odds_\theta(y_w \mid x)}{odds_\theta(y_l \mid x)}
$$</p>
<p>This ratio directly compares how much more likely the model is to generate the favored response compared to the disfavored one. By optimizing this ratio, the model learns to increase the likelihood of generating preferred responses.</p>
<h4 id="loss-function-combining-sft-and-preference-alignment"><strong>Loss Function: Combining SFT and Preference Alignment</strong></h4>
<p>The ORPO objective function combines the standard SFT loss (negative log-likelihood) with a penalty term based on the odds ratio. The SFT loss is the standard negative log-likelihood:</p>
<ol>
<li><strong>Standard SFT Loss</strong> (negative log-likelihood):</li>
</ol>
<p>$$
L_{SFT} = -\log P_{\theta}(y_w \mid x)
$$</p>
<ol start="2">
<li>
<p><strong>Odds Ratio-based Penalty</strong>:</p>
<p>$$
L_{OR} = -\log \sigma \left( \log \frac{odds_{\theta}(y_w \mid x)}{odds_{\theta}(y_l \mid x)} \right)
$$</p>
</li>
</ol>
<p>The penalty term L_OR is based on the log odds ratio, adjusted by a sigmoid function to smooth the gradient.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*Ec7s971dm0c89UIrPGc98w.png" alt="Three stages of LLM Training (i) Pretraining (ii) Supervised Finetuning and (iii) RLHF/preference optimization. Source —"></p>
<p>This term penalizes the model if the odds ratio between the chosen and disfavored responses is not sufficiently large.</p>
<p>The overall loss function is weighted sum of the two terms:</p>
<p>$$
L_{ORPO} = \mathbb{E}_{(x, y_w, y_l)} \left[ L_{SFT} + \lambda \cdot L_{OR} \right]
$$</p>
<p>The objective combines the standard SFT loss with a new term that penalizes the model if it fails to differentiate sufficiently between favored and disfavored responses. The hyperparameter λ controls the strength of this penalty.</p>
<h2 id="results">Results</h2>
<p><img src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*u-H5TO6hxL5rgfGTdXkuxQ.png" alt="Comparison of RLHF, DPO, and ORPO"></p>
<p>The authors performed various experiments on multiple LLMs ranging for 125M parameters to 7B parameter models. This includes Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B). ORPO consistently improved performance, surpassing state-of-the-art models with significantly larger parameters. ORPO-tuned models, such as Mistral-ORPO-α (7B) and Mistral-ORPO-β (7B), achieved up to 12.20% on AlpacaEval2.0. In the MT-Bench multi-turn instruction-following benchmark, Mistral-ORPO-α (7B) and Mistral-ORPO-β (7B) scored 7.23 and 7.32, respectively, which are comparable to or better than a few larger models. They also performed experiments to evaluate diversity of response. ORPO maintained a good balance between per-input diversity (low within-input variation) and across-input diversity (high between-input variation).</p>
<p>In this article we will look a new LLM preference optimization approach <a href="https://arxiv.org/pdf/2403.07691">ORPO: Monolithic Preference Optimization without Reference Model</a>. Do checkout my other articles on LLMs, including Self-Extend, GQA, etc.</p>
<h2 id="references">References</h2>
<ol>
<li>ORPO: Monolithic Preference Optimization without Reference Model — <a href="https://arxiv.org/abs/2403.07691">https://arxiv.org/abs/2403.07691</a></li>
<li>Training language models to follow instructions with human feedback — <a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></li>
</ol>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="">Reward Modeling</a>
  
  <a class="badge badge-light" href="">Machine Learning</a>
  
  <a class="badge badge-light" href="">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/llm/">LLM</a>
  
  <a class="badge badge-light" href="/tags/optimization/">Optimization</a>
  
  <a class="badge badge-light" href="/tags/ai/">AI</a>
  
</div>




    
      






  
  
    
  
  







      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/selfextend/">Tuning-Free Longer Context Lengths For LLMs — A Review of Self-Extend (LLM Maybe LongLM)</a></li>
          
          <li><a href="/post/gqa/">Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training</a></li>
          
          <li><a href="/post/lora/">Understanding LoRA — Low Rank Adaptation For Finetuning Large Models</a></li>
          
          <li><a href="/post/avrotf/">Linkedin opensources Avro2TF</a></li>
          
          <li><a href="/publication/conan/">CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion *(Best Paper Award)* (Oral)</a></li>
          
        </ul>
      </div>
      
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "bhavinjawade-me" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <form style="text-align: center;" id="formid" method="GET" enctype="application/x-www-form-urlencoded" action = "https://docs.google.com/forms/d/e/1FAIpQLScIS1hIVJyfo1iprQSLmZfKTj9zugkv7X2Ce6kOBaT1F4KChQ/formResponse">
        <input type = "hidden" name="entry.1639441032" id="firstname" value = "User"/>
        <input type = "hidden" name="entry.1590194600" id="lastname" value = ""/>
        <input name="entry.525161483" required placeholder="email address" id="email" type="email" style="width: 60%;outline: none;font-size: 18px;padding: 1%;font-weight: 500;color: #6d6d6d;border-radius: 29px;padding-left: 3%;padding-right: 3%;border: none;border: 1px solid #e2e2e2;border-top-right-radius: 0;border-bottom-right-radius: 0;"/>
        <input type="submit" name = "submit" value="Subscribe" style="margin-left: -5px;background-color: #58baff;border: none;border: 1px solid #58baf8;border-radius: 20px;padding: 1%;padding-left: 3%;padding-right: 3%;border-radius: 29px;font-size: 18px;color: white;cursor: pointer;border-top-left-radius: 0;border-bottom-left-radius: 0;"/>
    </form>
    <script src="http://code.jquery.com/jquery-1.9.1.js"></script>
    <script src="http://malsup.github.com/jquery.form.js"></script> 
    <script type='text/javascript'>
         
        $(document).ready(function() { 
                
                $('#formid').ajaxForm(function() { 
                    alert("Thank you for your comment!"); 
                });
            }); 
    </script>
  <br>
  <p class="powered-by">
    

    Copyright
    <a href="https://www.linkedin.com/in/bhavinjawade" target="_blank" rel="noopener">Bhavin Jawade</a> | 2018-2019 |  
    <a href="mailto:bhavinjawade@gmail.com" target="_blank" rel="noopener">bhavinjawade@gmail.com</a>.
    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
  
<script type="text/javascript">
    var vglnk = {key: '42643b2b96638249d923f00eda66c144'};
    (function(d, t) {
        var s = d.createElement(t);
            s.type = 'text/javascript';
            s.async = true;
            s.src = '//cdn.viglink.com/api/vglnk.js';
        var r = d.getElementsByTagName(t)[0];
            r.parentNode.insertBefore(s, r);
    }(document, 'script'));
</script>

</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//bhavinjawade-me.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.1846cc5c27f3006b56814095ec02281f.js"></script>

  </body>
</html>

